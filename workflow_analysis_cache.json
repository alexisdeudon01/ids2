{
  "h.py": {
    "hash": "c2c2e24fcaeb7d28d7b1fd15266cb538",
    "data": {
      "role": "utility",
      "description": "Generates an interactive HTML dependency graph visualization for Python/shell scripts in the project using Plotly and NetworkX. Analyzes file dependencies and creates a visual network diagram.",
      "purpose": "Provides visual dependency analysis and project structure understanding through interactive graph generation",
      "execution_order": 85,
      "is_executable": true,
      "is_standalone": true,
      "calls": [
        {
          "file": "graph_interactif.html",
          "method": "exec",
          "when": "runtime",
          "line": 87,
          "critical": false
        }
      ],
      "called_by_likely": [
        "main analysis scripts",
        "documentation generators",
        "project maintenance tools"
      ],
      "requires_before": [
        "project files to analyze must exist"
      ],
      "produces_after": [
        "graph_interactif.html enables manual dependency review"
      ],
      "category": "util",
      "inputs": [
        "project directory structure",
        "Python/shell/bash files",
        "plotly",
        "networkx",
        "pandas libraries"
      ],
      "outputs": [
        "graph_interactif.html interactive visualization file"
      ],
      "potential_bugs": [
        {
          "type": "missing_dependency",
          "description": "No error handling for missing plotly, networkx, or pandas imports",
          "severity": "high",
          "line": 1,
          "suggestion": "Add try-except blocks around imports with informative error messages",
          "file": "h.py"
        },
        {
          "type": "no_error_handling",
          "description": "File reading uses bare except clause that silently ignores all errors",
          "severity": "medium",
          "line": 26,
          "suggestion": "Replace bare except with specific exceptions and add logging",
          "file": "h.py"
        },
        {
          "type": "hardcoded_path",
          "description": "PROJECT_DIR hardcoded to '.' which may not work in all execution contexts",
          "severity": "medium",
          "line": 7,
          "suggestion": "Use __file__ path or accept PROJECT_DIR as command line argument",
          "file": "h.py"
        },
        {
          "type": "unreachable",
          "description": "Syntax error on line 81: 'annotations=,' has trailing comma with no value",
          "severity": "critical",
          "line": 81,
          "suggestion": "Remove trailing comma or provide annotations value",
          "file": "h.py"
        },
        {
          "type": "dead_code",
          "description": "pandas imported but never used in the code",
          "severity": "low",
          "line": 4,
          "suggestion": "Remove unused pandas import",
          "file": "h.py"
        },
        {
          "type": "missing_error_handling",
          "description": "No validation that output directory is writable before creating HTML file",
          "severity": "medium",
          "line": 87,
          "suggestion": "Check write permissions and handle file creation errors",
          "file": "h.py"
        }
      ],
      "fsm_state": {
        "state_name": "ANALYZING",
        "preconditions": [
          "project files exist",
          "required Python libraries installed",
          "current directory is project root"
        ],
        "postconditions": [
          "interactive HTML graph file created",
          "dependency relationships visualized"
        ],
        "can_fail": true,
        "on_failure": "abort",
        "timeout_seconds": 120
      }
    }
  },
  "07-install-pigpio.sh": {
    "hash": "8c1d5fb5019d30286d081cfa3b056027",
    "data": {
      "role": "utility",
      "description": "Downloads, compiles, and installs pigpio library from source code with systemd service setup for GPIO control on Raspberry Pi.",
      "purpose": "Provides GPIO control functionality by building pigpio from source for Debian Trixie compatibility when package manager versions are unavailable",
      "execution_order": 7,
      "is_executable": true,
      "is_standalone": true,
      "calls": [
        {
          "file": "pigpio.zip",
          "method": "exec",
          "when": "runtime",
          "line": 17,
          "critical": true
        },
        {
          "file": "/etc/systemd/system/pigpiod.service",
          "method": "exec",
          "when": "conditional",
          "line": 42,
          "critical": false
        },
        {
          "file": "Makefile",
          "method": "exec",
          "when": "runtime",
          "line": 28,
          "critical": true
        }
      ],
      "called_by_likely": [
        "00-main-installer.sh",
        "setup.sh",
        "install-dependencies.sh"
      ],
      "requires_before": [
        "01-update-system.sh",
        "package manager updates"
      ],
      "produces_after": [
        "GPIO control scripts",
        "pigpio-dependent applications",
        "hardware interface scripts"
      ],
      "category": "install",
      "inputs": [
        "internet connection",
        "PIGPIO_ARCHIVE_URL environment variable",
        "apt package manager",
        "systemd (optional)",
        "build tools"
      ],
      "outputs": [
        "pigpio library binaries",
        "pigpiod daemon",
        "systemd service file",
        "GPIO control capability"
      ],
      "potential_bugs": [
        {
          "type": "missing_dependency",
          "description": "Script assumes apt-get is available but doesn't verify package manager type",
          "severity": "high",
          "line": 8,
          "suggestion": "Check if apt-get exists before using it",
          "file": "07-install-pigpio.sh"
        },
        {
          "type": "no_error_handling",
          "description": "Network download has no retry mechanism or timeout handling",
          "severity": "medium",
          "line": 17,
          "suggestion": "Add wget timeout and retry options",
          "file": "07-install-pigpio.sh"
        },
        {
          "type": "race_condition",
          "description": "Multiple instances could create conflicting temp directories or systemd services",
          "severity": "medium",
          "line": 11,
          "suggestion": "Add file locking mechanism",
          "file": "07-install-pigpio.sh"
        },
        {
          "type": "hardcoded_path",
          "description": "Hardcoded /etc/systemd/system path may not exist on all systems",
          "severity": "medium",
          "line": 42,
          "suggestion": "Check if systemd directory exists before creating service",
          "file": "07-install-pigpio.sh"
        },
        {
          "type": "missing_dependency",
          "description": "No verification that system supports GPIO hardware before installing",
          "severity": "low",
          "line": 1,
          "suggestion": "Add hardware compatibility check",
          "file": "07-install-pigpio.sh"
        },
        {
          "type": "no_error_handling",
          "description": "find command could fail silently if extraction creates unexpected structure",
          "severity": "medium",
          "line": 20,
          "suggestion": "Add more robust directory detection with explicit error messages",
          "file": "07-install-pigpio.sh"
        },
        {
          "type": "missing_env",
          "description": "DEBIAN_FRONTEND export may not persist in all shell environments",
          "severity": "low",
          "line": 4,
          "suggestion": "Set DEBIAN_FRONTEND inline with apt commands",
          "file": "07-install-pigpio.sh"
        }
      ],
      "fsm_state": {
        "state_name": "INSTALLING_GPIO_LIBRARY",
        "preconditions": [
          "system updated",
          "internet connectivity",
          "root privileges",
          "build tools available"
        ],
        "postconditions": [
          "pigpio library installed",
          "pigpiod daemon available",
          "GPIO control enabled",
          "systemd service configured"
        ],
        "can_fail": true,
        "on_failure": "abort",
        "timeout_seconds": 300
      }
    }
  },
  "ko.py": {
    "hash": "ad6c66ea1f44fe5aca32bb71ab0666a2",
    "data": {
      "role": "entry_point",
      "description": "Analyzes script dependencies using graph theory and executes them in topological order. Scans project files for dependencies and runs them hierarchically.",
      "purpose": "Orchestrates project execution by automatically determining and enforcing correct script execution order based on file dependencies",
      "execution_order": 5,
      "is_executable": true,
      "is_standalone": true,
      "calls": [
        {
          "file": "*.py",
          "method": "subprocess",
          "when": "runtime",
          "line": 59,
          "critical": true
        },
        {
          "file": "*.sh",
          "method": "subprocess",
          "when": "runtime",
          "line": 59,
          "critical": true
        },
        {
          "file": "*.bash",
          "method": "subprocess",
          "when": "runtime",
          "line": 59,
          "critical": true
        }
      ],
      "called_by_likely": [
        "main orchestrator",
        "CI/CD pipeline",
        "deployment script"
      ],
      "requires_before": [
        "project files to exist in current directory"
      ],
      "produces_after": [
        "all discovered scripts executed in dependency order"
      ],
      "category": "core",
      "inputs": [
        "PROJECT_DIR filesystem",
        "*.py, *.sh, *.bash files",
        "PYTHONPATH environment variable"
      ],
      "outputs": [
        "execution results",
        "modified PYTHONPATH",
        "stdout/stderr from executed scripts"
      ],
      "potential_bugs": [
        {
          "type": "hardcoded_path",
          "description": "PROJECT_DIR hardcoded to '.' - will fail if run from different directory",
          "severity": "high",
          "line": 8,
          "suggestion": "Use os.path.dirname(__file__) or pass as argument",
          "file": "ko.py"
        },
        {
          "type": "no_error_handling",
          "description": "find_dependencies() has bare except clause hiding all errors including syntax errors",
          "severity": "medium",
          "line": 35,
          "suggestion": "Catch specific exceptions like IOError, UnicodeDecodeError",
          "file": "ko.py"
        },
        {
          "type": "race_condition",
          "description": "Scripts executed sequentially but may create/modify same files without locks",
          "severity": "medium",
          "line": 75,
          "suggestion": "Add file locking or check for concurrent access",
          "file": "ko.py"
        },
        {
          "type": "missing_dependency",
          "description": "Requires networkx but no dependency check - will crash if not installed",
          "severity": "critical",
          "line": 5,
          "suggestion": "Add try/except for import or requirements.txt check",
          "file": "ko.py"
        },
        {
          "type": "hardcoded_path",
          "description": "Hardcoded excluded script names may not match actual filenames in different environments",
          "severity": "low",
          "line": 13,
          "suggestion": "Move exclusions to config file or command line args",
          "file": "ko.py"
        },
        {
          "type": "missing_env",
          "description": "Assumes python3 command exists - may fail on systems with different Python installations",
          "severity": "medium",
          "line": 54,
          "suggestion": "Use sys.executable or check for python3 availability",
          "file": "ko.py"
        },
        {
          "type": "unreachable",
          "description": "Success message only prints if ALL scripts succeed, making partial success invisible",
          "severity": "low",
          "line": 91,
          "suggestion": "Add progress reporting for each successful script",
          "file": "ko.py"
        }
      ],
      "fsm_state": {
        "state_name": "ORCHESTRATING",
        "preconditions": [
          "project directory exists",
          "script files present",
          "networkx available",
          "python3/bash executables available"
        ],
        "postconditions": [
          "all scripts executed in dependency order",
          "PYTHONPATH modified",
          "execution results captured"
        ],
        "can_fail": true,
        "on_failure": "abort",
        "timeout_seconds": 300
      }
    }
  },
  "project_manager.py": {
    "hash": "222acf4b5cf9bc159f85543977dd606b",
    "data": {
      "role": "utility",
      "description": "Unified project management script that detects Python dependencies, verifies system software, executes scripts hierarchically, and generates interactive dependency graphs.",
      "purpose": "Central tool to analyze, manage and orchestrate project dependencies and script execution order",
      "execution_order": 15,
      "is_executable": true,
      "is_standalone": true,
      "calls": [
        {
          "file": "requirements.txt",
          "method": "source",
          "when": "runtime",
          "line": 150,
          "critical": false
        },
        {
          "file": "*.py",
          "method": "source",
          "when": "runtime",
          "line": 85,
          "critical": false
        },
        {
          "file": "*.sh",
          "method": "source",
          "when": "runtime",
          "line": 85,
          "critical": false
        }
      ],
      "called_by_likely": [
        "main.py",
        "setup.py",
        "build scripts",
        "CI/CD pipelines"
      ],
      "requires_before": [
        "project files exist",
        "python3 available"
      ],
      "produces_after": [
        "requirements.txt",
        "dependency analysis",
        "system verification",
        "HTML graph"
      ],
      "category": "util",
      "inputs": [
        "project files (*.py, *.sh)",
        "system commands",
        "environment variables"
      ],
      "outputs": [
        "requirements.txt",
        "dependency reports",
        "HTML interactive graph",
        "system status"
      ],
      "potential_bugs": [
        {
          "type": "missing_error_handling",
          "description": "File reading operations lack comprehensive error handling",
          "severity": "medium",
          "line": 95,
          "suggestion": "Add try-catch blocks around all file I/O operations",
          "file": "project_manager.py"
        },
        {
          "type": "hardcoded_path",
          "description": "PROJECT_DIR hardcoded to '.' may fail in different execution contexts",
          "severity": "medium",
          "line": 19,
          "suggestion": "Use os.path.dirname(__file__) or make configurable",
          "file": "project_manager.py"
        },
        {
          "type": "no_error_handling",
          "description": "subprocess calls for system commands lack error handling",
          "severity": "high",
          "line": 150,
          "suggestion": "Add try-catch around subprocess calls with proper error messages",
          "file": "project_manager.py"
        },
        {
          "type": "missing_dependency",
          "description": "Script assumes python3 and pip are available without verification first",
          "severity": "high",
          "line": 25,
          "suggestion": "Check system dependencies before attempting operations",
          "file": "project_manager.py"
        },
        {
          "type": "race_condition",
          "description": "Multiple scripts could modify requirements.txt simultaneously",
          "severity": "low",
          "line": 130,
          "suggestion": "Add file locking mechanism",
          "file": "project_manager.py"
        },
        {
          "type": "unreachable",
          "description": "Code appears truncated, install_requirements function incomplete",
          "severity": "critical",
          "line": 180,
          "suggestion": "Complete the implementation of install_requirements function",
          "file": "project_manager.py"
        },
        {
          "type": "hardcoded_path",
          "description": "requirements.txt filename hardcoded throughout",
          "severity": "low",
          "line": 130,
          "suggestion": "Make requirements file path configurable",
          "file": "project_manager.py"
        },
        {
          "type": "missing_env",
          "description": "VIRTUAL_ENV check may not work in all Python environments",
          "severity": "medium",
          "line": 175,
          "suggestion": "Add alternative virtual environment detection methods",
          "file": "project_manager.py"
        }
      ],
      "fsm_state": {
        "state_name": "ANALYZING",
        "preconditions": [
          "project files exist",
          "python3 interpreter available",
          "read permissions on project directory"
        ],
        "postconditions": [
          "dependencies analyzed",
          "requirements.txt generated",
          "system tools verified",
          "dependency graph created"
        ],
        "can_fail": true,
        "on_failure": "abort",
        "timeout_seconds": 300
      }
    }
  },
  "execution_flow.py": {
    "hash": "b5d072463569f6fca38220cf2fb43efa",
    "data": {
      "role": "utility",
      "description": "An intelligent execution flow analyzer that uses AI (Claude) to analyze Python/shell scripts, determine their execution order, dependencies, and generates interactive HTML visualization graphs of project structure.",
      "purpose": "To provide automated analysis of complex codebases, helping developers understand script dependencies, execution flows, and entry points through AI-powered static analysis",
      "execution_order": 75,
      "is_executable": true,
      "is_standalone": true,
      "calls": [
        {
          "file": "networkx",
          "method": "import",
          "when": "startup",
          "line": 32,
          "critical": true
        },
        {
          "file": "anthropic",
          "method": "import",
          "when": "startup",
          "line": 33,
          "critical": true
        },
        {
          "file": "pyvis.network",
          "method": "import",
          "when": "startup",
          "line": 34,
          "critical": true
        },
        {
          "file": "psutil",
          "method": "import",
          "when": "startup",
          "line": 11,
          "critical": true
        },
        {
          "file": "execution_flow_cache.json",
          "method": "source",
          "when": "runtime",
          "line": 44,
          "critical": false
        },
        {
          "file": "execution_flow.html",
          "method": "exec",
          "when": "runtime",
          "line": 45,
          "critical": false
        }
      ],
      "called_by_likely": [
        "main analysis scripts",
        "CI/CD pipelines",
        "documentation generators",
        "development workflow scripts"
      ],
      "requires_before": [
        "pip install requirements",
        "ANTHROPIC_API_KEY environment setup"
      ],
      "produces_after": [
        "visualization HTML files",
        "dependency analysis reports",
        "execution flow documentation"
      ],
      "category": "util",
      "inputs": [
        "ANTHROPIC_API_KEY environment variable",
        "target directory path (/home/tor/Downloads/ids2)",
        "Python/shell/YAML files in target directory",
        "execution_flow_cache.json (optional)"
      ],
      "outputs": [
        "execution_flow.html interactive graph",
        "execution_flow_cache.json cache file",
        "console analysis reports",
        "system resource monitoring output"
      ],
      "potential_bugs": [
        {
          "type": "hardcoded_path",
          "description": "Root directory hardcoded to '/home/tor/Downloads/ids2' - will fail on other systems",
          "severity": "high",
          "line": 43,
          "suggestion": "Make root_dir configurable via command line argument or environment variable",
          "file": "execution_flow.py"
        },
        {
          "type": "missing_env",
          "description": "ANTHROPIC_API_KEY has hardcoded fallback that may be invalid/expired, no validation",
          "severity": "critical",
          "line": 41,
          "suggestion": "Validate API key existence and format, fail gracefully if missing",
          "file": "execution_flow.py"
        },
        {
          "type": "missing_dependency",
          "description": "Critical imports (networkx, anthropic, pyvis) cause sys.exit(1) if missing, but error message is incomplete",
          "severity": "medium",
          "line": 35,
          "suggestion": "Add specific package versions and complete installation command",
          "file": "execution_flow.py"
        },
        {
          "type": "race_condition",
          "description": "ResourceMonitor uses threading with shared state (_stats) - potential race conditions despite locking",
          "severity": "medium",
          "line": 141,
          "suggestion": "Use thread-safe data structures or atomic operations",
          "file": "execution_flow.py"
        },
        {
          "type": "no_error_handling",
          "description": "No try/catch around file I/O operations for cache and HTML output",
          "severity": "medium",
          "line": 44,
          "suggestion": "Add exception handling for file operations with graceful degradation",
          "file": "execution_flow.py"
        },
        {
          "type": "missing_dependency",
          "description": "Model name 'claude-sonnet-4-20250514' may not exist or be accessible",
          "severity": "high",
          "line": 42,
          "suggestion": "Use a known stable model name or validate model availability",
          "file": "execution_flow.py"
        },
        {
          "type": "hardcoded_path",
          "description": "Output files (cache, HTML) use relative paths - could write to unexpected locations",
          "severity": "medium",
          "line": 44,
          "suggestion": "Use absolute paths or explicit output directory configuration",
          "file": "execution_flow.py"
        },
        {
          "type": "resource_leak",
          "description": "ThreadPoolExecutor and network connections may not be properly closed on exceptions",
          "severity": "medium",
          "line": 11,
          "suggestion": "Use context managers (with statements) for resource management",
          "file": "execution_flow.py"
        }
      ],
      "fsm_state": {
        "state_name": "ANALYZING",
        "preconditions": [
          "target directory exists and is readable",
          "required Python packages installed",
          "ANTHROPIC_API_KEY available",
          "sufficient system resources"
        ],
        "postconditions": [
          "dependency graph generated",
          "HTML visualization created",
          "cache file updated",
          "analysis report displayed"
        ],
        "can_fail": true,
        "on_failure": "abort",
        "timeout_seconds": 300
      }
    }
  },
  "workflow_analyzer.py": {
    "hash": "6af0f75d97fb4df620d931308bd724eb",
    "data": {
      "role": "utility",
      "description": "An advanced workflow analyzer that uses AI to analyze project files, detect bugs, generate finite state machines, and create interactive HTML reports. It combines static code analysis with resource monitoring and parallel processing.",
      "purpose": "To provide comprehensive project analysis, bug detection, workflow visualization, and FSM generation for understanding complex codebases and their execution patterns",
      "execution_order": 15,
      "is_executable": true,
      "is_standalone": true,
      "calls": [
        {
          "file": "anthropic",
          "method": "import",
          "when": "startup",
          "line": 32,
          "critical": true
        },
        {
          "file": "networkx",
          "method": "import",
          "when": "startup",
          "line": 31,
          "critical": true
        },
        {
          "file": "psutil",
          "method": "import",
          "when": "runtime",
          "line": 21,
          "critical": true
        },
        {
          "file": "workflow_analysis_cache.json",
          "method": "source",
          "when": "startup",
          "line": 50,
          "critical": false
        },
        {
          "file": "workflow_analysis.html",
          "method": "exec",
          "when": "runtime",
          "line": 51,
          "critical": false
        }
      ],
      "called_by_likely": [
        "main.py",
        "analyze.py",
        "setup.py",
        "development scripts"
      ],
      "requires_before": [
        "pip install networkx anthropic psutil"
      ],
      "produces_after": [
        "workflow_analysis.html",
        "workflow_analysis_cache.json",
        "analysis reports"
      ],
      "category": "util",
      "inputs": [
        "ANTHROPIC_API_KEY environment variable or hardcoded key",
        "root_dir directory to analyze (/home/tor/Downloads/ids2)",
        "Project files with extensions: .py, .sh, .yml, .yaml, .service, .bash",
        "Special files: Dockerfile, Makefile, docker-compose.yml"
      ],
      "outputs": [
        "workflow_analysis.html - Interactive multi-tab HTML report",
        "workflow_analysis_cache.json - Cached analysis results",
        "Console output with colored logging",
        "Browser window (auto-opened)"
      ],
      "potential_bugs": [
        {
          "type": "hardcoded_path",
          "description": "Hardcoded API key exposed in source code",
          "severity": "critical",
          "line": 46,
          "suggestion": "Move API key to environment variable or config file, never commit secrets to code",
          "file": "workflow_analyzer.py"
        },
        {
          "type": "hardcoded_path",
          "description": "Hardcoded root directory path '/home/tor/Downloads/ids2' won't work on other systems",
          "severity": "high",
          "line": 48,
          "suggestion": "Use os.getcwd() or accept path as command line argument",
          "file": "workflow_analyzer.py"
        },
        {
          "type": "missing_dependency",
          "description": "Missing error handling for failed imports - script exits immediately",
          "severity": "medium",
          "line": 33,
          "suggestion": "Add graceful degradation or better error messages for missing dependencies",
          "file": "workflow_analyzer.py"
        },
        {
          "type": "race_condition",
          "description": "ThreadPoolExecutor with shared resource monitoring could cause race conditions",
          "severity": "medium",
          "line": 21,
          "suggestion": "Ensure proper thread synchronization for shared state access",
          "file": "workflow_analyzer.py"
        },
        {
          "type": "no_error_handling",
          "description": "No validation of API key format or network connectivity before making API calls",
          "severity": "high",
          "line": 46,
          "suggestion": "Add API key validation and network error handling",
          "file": "workflow_analyzer.py"
        },
        {
          "type": "missing_env",
          "description": "API key hardcoded instead of using environment variable",
          "severity": "critical",
          "line": 46,
          "suggestion": "Use os.getenv('ANTHROPIC_API_KEY') with fallback or error",
          "file": "workflow_analyzer.py"
        },
        {
          "type": "resource_leak",
          "description": "ThreadPoolExecutor and monitoring threads may not clean up properly on exit",
          "severity": "medium",
          "line": 150,
          "suggestion": "Add proper cleanup in finally blocks or context managers",
          "file": "workflow_analyzer.py"
        }
      ],
      "fsm_state": {
        "state_name": "ANALYZING",
        "preconditions": [
          "Python 3.x installed",
          "Required packages installed (networkx, anthropic, psutil)",
          "Target directory exists and is readable",
          "Valid Anthropic API key available"
        ],
        "postconditions": [
          "HTML analysis report generated",
          "Cache file updated",
          "Bug analysis completed",
          "FSM visualizations created",
          "Browser opened to results"
        ],
        "can_fail": true,
        "on_failure": "abort",
        "timeout_seconds": 300
      }
    }
  },
  "hh.py": {
    "hash": "9abe49dd54434d1ec1f3a13b5a1955e6",
    "data": {
      "role": "utility",
      "description": "This script analyzes Python source code in a project to identify external dependencies and suggests requirements.txt content. It walks through Python files, extracts import statements, and filters out built-in and local modules.",
      "purpose": "To automatically discover external Python package dependencies in a project for requirements.txt generation",
      "execution_order": 25,
      "is_executable": true,
      "is_standalone": true,
      "calls": [
        {
          "file": "*.py",
          "method": "import",
          "when": "runtime",
          "line": 23,
          "critical": false
        },
        {
          "file": "requirements.txt",
          "method": "source",
          "when": "runtime",
          "line": 58,
          "critical": false
        }
      ],
      "called_by_likely": [
        "setup.py",
        "build scripts",
        "development maintenance scripts"
      ],
      "requires_before": [
        "Python source files to analyze"
      ],
      "produces_after": [
        "requirements.txt suggestions",
        "dependency documentation"
      ],
      "category": "util",
      "inputs": [
        "Python source files in current directory tree",
        "PROJECT_DIR path",
        "File system access"
      ],
      "outputs": [
        "Console output listing external dependencies",
        "Requirements.txt suggestions"
      ],
      "potential_bugs": [
        {
          "type": "hardcoded_path",
          "description": "PROJECT_DIR hardcoded to '.' - may fail if run from different directory",
          "severity": "medium",
          "line": 6,
          "suggestion": "Use os.path.dirname(__file__) or accept path as parameter",
          "file": "hh.py"
        },
        {
          "type": "no_error_handling",
          "description": "File reading has basic exception handling but continues processing - could miss critical errors",
          "severity": "low",
          "line": 33,
          "suggestion": "Add more specific error handling and logging",
          "file": "hh.py"
        },
        {
          "type": "missing_dependency",
          "description": "Hardcoded list of standard library modules may be incomplete for different Python versions",
          "severity": "medium",
          "line": 30,
          "suggestion": "Use stdlib_list package or check sys.stdlib_module_names",
          "file": "hh.py"
        },
        {
          "type": "unreachable",
          "description": "The script excludes itself and specific test files but uses hardcoded names",
          "severity": "low",
          "line": 15,
          "suggestion": "Make exclusion list configurable or use pattern matching",
          "file": "hh.py"
        },
        {
          "type": "race_condition",
          "description": "No protection against files being modified during analysis",
          "severity": "low",
          "line": 28,
          "suggestion": "Add file modification time checks or locking",
          "file": "hh.py"
        },
        {
          "type": "missing_env",
          "description": "No validation that PROJECT_DIR exists or is readable",
          "severity": "medium",
          "line": 6,
          "suggestion": "Add directory existence and permission checks",
          "file": "hh.py"
        }
      ],
      "fsm_state": {
        "state_name": "ANALYZING",
        "preconditions": [
          "Python source files exist in project",
          "Read permissions on source directory",
          "Python interpreter available"
        ],
        "postconditions": [
          "External dependencies identified",
          "Requirements suggestions generated",
          "Analysis report displayed"
        ],
        "can_fail": true,
        "on_failure": "skip",
        "timeout_seconds": 30
      }
    }
  },
  "unified_executor.py": {
    "hash": "d39b8a6017a7049d608f57effca6575c",
    "data": {
      "role": "entry_point",
      "description": "A unified execution orchestrator that runs all project scripts in optimal order with FSM state management, error handling, and retry logic.",
      "purpose": "Provides a single entry point to execute the entire project workflow with proper dependency ordering, timeout management, and failure handling",
      "execution_order": 1,
      "is_executable": true,
      "is_standalone": true,
      "calls": [
        {
          "file": "dynamic",
          "method": "subprocess",
          "when": "runtime",
          "line": 92,
          "critical": true
        },
        {
          "file": "unified_execution.log",
          "method": "exec",
          "when": "startup",
          "line": 20,
          "critical": false
        }
      ],
      "called_by_likely": [
        "main.py",
        "run.sh",
        "start.py",
        "deploy.py"
      ],
      "requires_before": [],
      "produces_after": [
        "all project scripts based on EXECUTION_STEPS configuration"
      ],
      "category": "core",
      "inputs": [
        "project_dir",
        "EXECUTION_STEPS configuration",
        "environment variables",
        "script files in project"
      ],
      "outputs": [
        "unified_execution.log",
        "execution state changes",
        "orchestrated script results"
      ],
      "potential_bugs": [
        {
          "type": "missing_dependency",
          "description": "EXECUTION_STEPS list is empty - no scripts will be executed",
          "severity": "critical",
          "line": 54,
          "suggestion": "Populate EXECUTION_STEPS with actual project scripts",
          "file": "unified_executor.py"
        },
        {
          "type": "hardcoded_path",
          "description": "Hardcoded log filename 'unified_execution.log' without path validation",
          "severity": "medium",
          "line": 21,
          "suggestion": "Use configurable log path with validation",
          "file": "unified_executor.py"
        },
        {
          "type": "race_condition",
          "description": "No file locking mechanism for concurrent executions",
          "severity": "high",
          "line": 65,
          "suggestion": "Add file locking to prevent multiple simultaneous executions",
          "file": "unified_executor.py"
        },
        {
          "type": "unreachable",
          "description": "Code after line 152 is cut off, method incomplete",
          "severity": "critical",
          "line": 152,
          "suggestion": "Complete the run() method implementation",
          "file": "unified_executor.py"
        },
        {
          "type": "no_error_handling",
          "description": "Missing signal handler cleanup despite importing signal",
          "severity": "medium",
          "line": 13,
          "suggestion": "Add proper signal handlers for graceful shutdown",
          "file": "unified_executor.py"
        },
        {
          "type": "missing_dependency",
          "description": "Assumes bash and python executables exist without checking",
          "severity": "medium",
          "line": 85,
          "suggestion": "Add executable existence checks before subprocess calls",
          "file": "unified_executor.py"
        },
        {
          "type": "hardcoded_path",
          "description": "PYTHONPATH hardcoded to project_dir might conflict with existing paths",
          "severity": "low",
          "line": 95,
          "suggestion": "Append to existing PYTHONPATH instead of overriding",
          "file": "unified_executor.py"
        }
      ],
      "fsm_state": {
        "state_name": "ORCHESTRATING",
        "preconditions": [
          "project directory exists",
          "EXECUTION_STEPS configured",
          "required scripts exist"
        ],
        "postconditions": [
          "all configured scripts executed",
          "execution log created",
          "final state reached"
        ],
        "can_fail": true,
        "on_failure": "abort",
        "timeout_seconds": 300
      }
    }
  },
  "generate_graph.py": {
    "hash": "0fd58a5da794956fe18d81820f9adba7",
    "data": {
      "role": "utility",
      "description": "Generates a visual dependency graph of scripts in a project using Graphviz, showing relationships between .py, .sh, .pl, and .bash files.",
      "purpose": "Provides visualization of script dependencies to understand project structure and execution flow for documentation or debugging purposes",
      "execution_order": 85,
      "is_executable": true,
      "is_standalone": true,
      "calls": [
        {
          "file": "*.py",
          "method": "source",
          "when": "runtime",
          "line": 42,
          "critical": false
        },
        {
          "file": "*.sh",
          "method": "source",
          "when": "runtime",
          "line": 42,
          "critical": false
        },
        {
          "file": "*.pl",
          "method": "source",
          "when": "runtime",
          "line": 42,
          "critical": false
        },
        {
          "file": "*.bash",
          "method": "source",
          "when": "runtime",
          "line": 42,
          "critical": false
        }
      ],
      "called_by_likely": [
        "documentation_scripts",
        "analysis_tools",
        "manual_execution"
      ],
      "requires_before": [
        "project_scripts_exist"
      ],
      "produces_after": [
        "visualization_tools",
        "documentation_generation"
      ],
      "category": "util",
      "inputs": [
        "project_directory_files",
        "graphviz_library",
        "script_files_with_extensions"
      ],
      "outputs": [
        "graphe_complet.png",
        "console_output",
        "dependency_graph_visualization"
      ],
      "potential_bugs": [
        {
          "type": "missing_dependency",
          "description": "graphviz library not imported with error handling - script will crash if not installed",
          "severity": "high",
          "line": 2,
          "suggestion": "Add try/except around graphviz import with user-friendly error message",
          "file": "generate_graph.py"
        },
        {
          "type": "no_error_handling",
          "description": "File operations lack proper error handling beyond bare except clause",
          "severity": "medium",
          "line": 19,
          "suggestion": "Replace bare except with specific exceptions and proper error reporting",
          "file": "generate_graph.py"
        },
        {
          "type": "hardcoded_path",
          "description": "PROJECT_DIR hardcoded to current directory, may not work in all contexts",
          "severity": "low",
          "line": 5,
          "suggestion": "Make PROJECT_DIR configurable via command line argument or environment variable",
          "file": "generate_graph.py"
        },
        {
          "type": "missing_dependency",
          "description": "graphviz system dependency not checked - dot.render() will fail if graphviz not installed on system",
          "severity": "critical",
          "line": 62,
          "suggestion": "Check for graphviz system installation before attempting to render",
          "file": "generate_graph.py"
        },
        {
          "type": "race_condition",
          "description": "File reading without locks could fail if files are being modified during scan",
          "severity": "low",
          "line": 19,
          "suggestion": "Consider adding file lock checks or handling concurrent modification gracefully",
          "file": "generate_graph.py"
        },
        {
          "type": "hardcoded_path",
          "description": "Output filename hardcoded as 'graphe_complet', could overwrite existing files",
          "severity": "low",
          "line": 61,
          "suggestion": "Add timestamp to filename or make output name configurable",
          "file": "generate_graph.py"
        },
        {
          "type": "unreachable",
          "description": "If roots is empty but all_scripts exists, the fallback makes all files roots but graph structure may be meaningless",
          "severity": "medium",
          "line": 37,
          "suggestion": "Better circular dependency detection and user warning about graph validity",
          "file": "generate_graph.py"
        }
      ],
      "fsm_state": {
        "state_name": "ANALYZING",
        "preconditions": [
          "project_files_exist",
          "graphviz_installed",
          "current_directory_accessible"
        ],
        "postconditions": [
          "dependency_graph_created",
          "png_file_generated",
          "relationships_visualized"
        ],
        "can_fail": true,
        "on_failure": "abort",
        "timeout_seconds": 300
      }
    }
  },
  "i.py": {
    "hash": "ad2bf0240af946d9332c0bb694ae61e6",
    "data": {
      "role": "utility",
      "description": "Generates a dependency graph visualization of project scripts using Graphviz. Analyzes .sh, .py, and .bash files to identify call relationships and creates a PNG diagram showing the project architecture.",
      "purpose": "Provides visual documentation of project structure and script dependencies to help developers understand execution flow and relationships",
      "execution_order": 95,
      "is_executable": true,
      "is_standalone": true,
      "calls": [
        {
          "file": "*.py",
          "method": "import",
          "when": "runtime",
          "line": 34,
          "critical": false
        },
        {
          "file": "*.sh",
          "method": "source",
          "when": "runtime",
          "line": 34,
          "critical": false
        },
        {
          "file": "*.bash",
          "method": "source",
          "when": "runtime",
          "line": 34,
          "critical": false
        }
      ],
      "called_by_likely": [
        "manual execution",
        "documentation generation scripts",
        "CI/CD pipeline"
      ],
      "requires_before": [
        "all project scripts must exist"
      ],
      "produces_after": [
        "documentation can be updated with generated graph"
      ],
      "category": "util",
      "inputs": [
        "project directory structure",
        "script files (.sh, .py, .bash)",
        "graphviz library"
      ],
      "outputs": [
        "mon_architecture_ids2.png",
        "dependency graph visualization",
        "console analysis summary"
      ],
      "potential_bugs": [
        {
          "type": "missing_dependency",
          "description": "graphviz library import without error handling",
          "severity": "high",
          "line": 3,
          "suggestion": "Add try-except around graphviz import with informative error message",
          "file": "i.py"
        },
        {
          "type": "no_error_handling",
          "description": "File operations lack comprehensive error handling",
          "severity": "medium",
          "line": 34,
          "suggestion": "Add specific exception handling for file reading operations",
          "file": "i.py"
        },
        {
          "type": "hardcoded_path",
          "description": "PROJECT_DIR hardcoded to current directory",
          "severity": "low",
          "line": 6,
          "suggestion": "Accept PROJECT_DIR as command line argument or environment variable",
          "file": "i.py"
        },
        {
          "type": "unreachable",
          "description": "Generic except clause hides all file reading errors",
          "severity": "medium",
          "line": 40,
          "suggestion": "Replace bare except with specific exceptions and logging",
          "file": "i.py"
        },
        {
          "type": "race_condition",
          "description": "File system changes during analysis could cause inconsistent results",
          "severity": "low",
          "line": 10,
          "suggestion": "Snapshot file list before analysis",
          "file": "i.py"
        },
        {
          "type": "missing_dependency",
          "description": "No validation that graphviz system dependency is installed",
          "severity": "medium",
          "line": 44,
          "suggestion": "Check graphviz availability before attempting render",
          "file": "i.py"
        },
        {
          "type": "hardcoded_path",
          "description": "Output filename is hardcoded",
          "severity": "low",
          "line": 78,
          "suggestion": "Make output filename configurable",
          "file": "i.py"
        }
      ],
      "fsm_state": {
        "state_name": "ANALYZING",
        "preconditions": [
          "project files exist",
          "graphviz library available",
          "read permissions on project directory"
        ],
        "postconditions": [
          "dependency graph image created",
          "analysis summary printed",
          "project structure documented"
        ],
        "can_fail": true,
        "on_failure": "abort",
        "timeout_seconds": 120
      }
    }
  },
  "w.py": {
    "hash": "f49867b24f2814620a4811260040d468",
    "data": {
      "role": "utility",
      "description": "Analyzes dependencies between project scripts and executes them in topological order based on their interdependencies. Uses NetworkX to detect circular dependencies and ensure proper execution sequence.",
      "purpose": "To automatically discover and execute project scripts in the correct dependency order, preventing execution failures due to missing prerequisites",
      "execution_order": 15,
      "is_executable": true,
      "is_standalone": true,
      "calls": [
        {
          "file": "*.py",
          "method": "subprocess",
          "when": "runtime",
          "line": 52,
          "critical": true
        },
        {
          "file": "*.sh",
          "method": "subprocess",
          "when": "runtime",
          "line": 49,
          "critical": true
        },
        {
          "file": "*.bash",
          "method": "subprocess",
          "when": "runtime",
          "line": 49,
          "critical": true
        }
      ],
      "called_by_likely": [
        "main orchestration scripts",
        "CI/CD pipelines",
        "deployment scripts"
      ],
      "requires_before": [
        "target scripts must exist in project directory"
      ],
      "produces_after": [
        "enables dependent scripts to run in correct order"
      ],
      "category": "util",
      "inputs": [
        "project directory with .py/.sh/.bash files",
        "networkx library",
        "python3 and bash interpreters"
      ],
      "outputs": [
        "execution logs",
        "success/failure status",
        "topological execution order"
      ],
      "potential_bugs": [
        {
          "type": "missing_dependency",
          "description": "NetworkX library not installed but no import error handling",
          "severity": "critical",
          "line": 5,
          "suggestion": "Add try/except around import nx with informative error message",
          "file": "w.py"
        },
        {
          "type": "hardcoded_path",
          "description": "PROJECT_DIR hardcoded to current directory, may fail in different contexts",
          "severity": "medium",
          "line": 8,
          "suggestion": "Accept PROJECT_DIR as command line argument or environment variable",
          "file": "w.py"
        },
        {
          "type": "no_error_handling",
          "description": "File reading in find_dependencies uses bare except clause, hiding real errors",
          "severity": "medium",
          "line": 32,
          "suggestion": "Catch specific exceptions (IOError, UnicodeDecodeError) and log them",
          "file": "w.py"
        },
        {
          "type": "race_condition",
          "description": "No file locking when reading files, could fail if files are being modified during execution",
          "severity": "low",
          "line": 25,
          "suggestion": "Add file locking or retry mechanism",
          "file": "w.py"
        },
        {
          "type": "missing_dependency",
          "description": "Assumes python3 and bash are available in PATH without checking",
          "severity": "high",
          "line": 49,
          "suggestion": "Check if interpreters exist before subprocess.run() calls",
          "file": "w.py"
        },
        {
          "type": "hardcoded_path",
          "description": "Hardcoded interpreter names 'python3' and 'bash' may not exist on all systems",
          "severity": "medium",
          "line": 49,
          "suggestion": "Use sys.executable for Python, check for bash/sh availability",
          "file": "w.py"
        },
        {
          "type": "missing_env",
          "description": "No validation that required environment variables are set for executed scripts",
          "severity": "medium",
          "line": 52,
          "suggestion": "Allow passing environment variables or validate prerequisites",
          "file": "w.py"
        },
        {
          "type": "dead_code",
          "description": "STDOUT print in error case may be empty and confusing",
          "severity": "low",
          "line": 59,
          "suggestion": "Only print STDOUT/STDERR if they contain content",
          "file": "w.py"
        },
        {
          "type": "unreachable",
          "description": "Success message after sys.exit(1) calls will never be reached if any script fails",
          "severity": "low",
          "line": 84,
          "suggestion": "This is actually correct behavior, not a bug",
          "file": "w.py"
        }
      ],
      "fsm_state": {
        "state_name": "ORCHESTRATING",
        "preconditions": [
          "project directory exists",
          "scripts have proper file extensions",
          "networkx library available",
          "python3/bash interpreters installed"
        ],
        "postconditions": [
          "all scripts executed in dependency order",
          "dependency graph validated",
          "execution results logged"
        ],
        "can_fail": true,
        "on_failure": "abort",
        "timeout_seconds": 300
      }
    }
  },
  "as.py": {
    "hash": "53b35a01f8f17f3b914f884c43aaf661",
    "data": {
      "role": "utility",
      "description": "An intelligent file dependency analyzer that uses Claude AI to scan project files and create an interactive dependency graph visualization. It analyzes Python, shell, YAML, and Docker files to map their relationships.",
      "purpose": "To automatically discover and visualize file dependencies in a project using AI analysis, helping developers understand code structure and relationships",
      "execution_order": 75,
      "is_executable": true,
      "is_standalone": true,
      "calls": [
        {
          "file": "analysis_cache.json",
          "method": "source",
          "when": "startup",
          "line": 20,
          "critical": false
        },
        {
          "file": "dependency_map_ai.html",
          "method": "exec",
          "when": "runtime",
          "line": 104,
          "critical": false
        },
        {
          "file": "files in ROOT_DIR",
          "method": "source",
          "when": "runtime",
          "line": 65,
          "critical": true
        }
      ],
      "called_by_likely": [
        "main.py",
        "analyze.py",
        "debug scripts",
        "project maintenance tools"
      ],
      "requires_before": [
        "pip install requirements",
        "anthropic API key setup",
        "target project files"
      ],
      "produces_after": [
        "dependency visualization tools",
        "project documentation scripts"
      ],
      "category": "util",
      "inputs": [
        "/home/tor/Downloads/ids2 directory",
        "ANTHROPIC_KEY environment/hardcoded",
        "analysis_cache.json",
        "Python/shell/YAML/Docker files"
      ],
      "outputs": [
        "analysis_cache.json",
        "dependency_map_ai.html",
        "console output",
        "NetworkX graph structure"
      ],
      "potential_bugs": [
        {
          "type": "hardcoded_path",
          "description": "ROOT_DIR is hardcoded to /home/tor/Downloads/ids2 which may not exist on other systems",
          "severity": "high",
          "line": 9,
          "suggestion": "Use command line argument or environment variable",
          "file": "as.py"
        },
        {
          "type": "missing_dependency",
          "description": "No error handling if anthropic, networkx, or pyvis packages are not installed",
          "severity": "medium",
          "line": 4,
          "suggestion": "Add try/catch for imports with helpful error messages",
          "file": "as.py"
        },
        {
          "type": "no_error_handling",
          "description": "File reading uses errors='ignore' which silently corrupts data",
          "severity": "medium",
          "line": 67,
          "suggestion": "Use proper encoding detection or handle encoding errors explicitly",
          "file": "as.py"
        },
        {
          "type": "hardcoded_path",
          "description": "API key is hardcoded in source code, major security risk",
          "severity": "critical",
          "line": 8,
          "suggestion": "Use environment variable or secure key management",
          "file": "as.py"
        },
        {
          "type": "missing_env",
          "description": "No validation that ANTHROPIC_KEY is valid or that API is accessible",
          "severity": "high",
          "line": 11,
          "suggestion": "Add API key validation and connectivity check",
          "file": "as.py"
        },
        {
          "type": "race_condition",
          "description": "Cache file operations not thread-safe, could corrupt cache if run in parallel",
          "severity": "medium",
          "line": 20,
          "suggestion": "Add file locking mechanism",
          "file": "as.py"
        },
        {
          "type": "no_error_handling",
          "description": "os.walk could fail on permission errors but not handled",
          "severity": "medium",
          "line": 64,
          "suggestion": "Wrap file system operations in try/catch",
          "file": "as.py"
        },
        {
          "type": "missing_dependency",
          "description": "Uses claude-sonnet-4-20250514 model which may not exist or be deprecated",
          "severity": "high",
          "line": 43,
          "suggestion": "Use current model names and add fallback options",
          "file": "as.py"
        },
        {
          "type": "no_error_handling",
          "description": "JSON parsing of Claude response could fail on malformed output",
          "severity": "medium",
          "line": 50,
          "suggestion": "Add validation for JSON structure before parsing",
          "file": "as.py"
        }
      ],
      "fsm_state": {
        "state_name": "ANALYZING",
        "preconditions": [
          "target directory exists",
          "anthropic API accessible",
          "required packages installed"
        ],
        "postconditions": [
          "dependency graph created",
          "cache updated",
          "HTML visualization generated"
        ],
        "can_fail": true,
        "on_failure": "skip",
        "timeout_seconds": 300
      }
    }
  },
  "aa.py": {
    "hash": "6519a9bfc94baca6cd4309a46514ccd9",
    "data": {
      "role": "utility",
      "description": "Analyzes code dependencies in a project directory using Claude AI and generates an interactive HTML dependency graph visualization.",
      "purpose": "To automatically discover and visualize file dependencies in a codebase using AI analysis rather than static parsing",
      "execution_order": 95,
      "is_executable": true,
      "is_standalone": true,
      "calls": [
        {
          "file": "ai_dependency_map.html",
          "method": "exec",
          "when": "runtime",
          "line": 74,
          "critical": false
        }
      ],
      "called_by_likely": [
        "manual execution",
        "analysis scripts",
        "documentation generators"
      ],
      "requires_before": [],
      "produces_after": [
        "ai_dependency_map.html"
      ],
      "category": "util",
      "inputs": [
        "/home/tor/Downloads/ids2 directory",
        "Anthropic API key",
        "files with extensions: .py, .sh, .yml, .yaml, .service, Dockerfile"
      ],
      "outputs": [
        "ai_dependency_map.html",
        "console output with analysis progress",
        "NetworkX graph structure"
      ],
      "potential_bugs": [
        {
          "type": "hardcoded_path",
          "description": "ROOT_DIR is hardcoded to /home/tor/Downloads/ids2 which may not exist on other systems",
          "severity": "high",
          "line": 8,
          "suggestion": "Use command line argument or environment variable for ROOT_DIR",
          "file": "aa.py"
        },
        {
          "type": "missing_env",
          "description": "Anthropic API key is hardcoded and exposed in source code",
          "severity": "critical",
          "line": 7,
          "suggestion": "Use environment variable ANTHROPIC_API_KEY instead of hardcoding",
          "file": "aa.py"
        },
        {
          "type": "no_error_handling",
          "description": "File reading uses errors='ignore' which silently fails on encoding issues",
          "severity": "medium",
          "line": 37,
          "suggestion": "Handle encoding errors explicitly and log warnings",
          "file": "aa.py"
        },
        {
          "type": "no_error_handling",
          "description": "No validation that ROOT_DIR exists before walking it",
          "severity": "medium",
          "line": 34,
          "suggestion": "Check if ROOT_DIR exists and is readable before os.walk()",
          "file": "aa.py"
        },
        {
          "type": "missing_dependency",
          "description": "NetworkX and pyvis imports not checked for availability",
          "severity": "medium",
          "line": 3,
          "suggestion": "Add try/except for imports and provide helpful error message",
          "file": "aa.py"
        },
        {
          "type": "race_condition",
          "description": "No check if ai_dependency_map.html is writable or if another process is using it",
          "severity": "low",
          "line": 74,
          "suggestion": "Check file permissions and handle file write conflicts",
          "file": "aa.py"
        },
        {
          "type": "no_error_handling",
          "description": "API response parsing assumes specific JSON structure without validation",
          "severity": "medium",
          "line": 27,
          "suggestion": "Validate JSON structure before accessing fields",
          "file": "aa.py"
        },
        {
          "type": "missing_dependency",
          "description": "Anthropic client creation doesn't verify API key validity",
          "severity": "medium",
          "line": 7,
          "suggestion": "Test API connection on initialization",
          "file": "aa.py"
        }
      ],
      "fsm_state": {
        "state_name": "ANALYZING",
        "preconditions": [
          "ROOT_DIR exists and is readable",
          "Anthropic API key is valid",
          "Required Python packages installed",
          "Network connectivity available"
        ],
        "postconditions": [
          "ai_dependency_map.html created",
          "Dependency graph analyzed",
          "Console output completed"
        ],
        "can_fail": true,
        "on_failure": "abort",
        "timeout_seconds": 300
      }
    }
  },
  "intelligent_analyzer.py": {
    "hash": "b2b18996dac5c3f32df8e719b60c0b9e",
    "data": {
      "role": "service",
      "description": "An intelligent dependency analyzer that scans Python projects, analyzes file dependencies using AI, and generates interactive HTML visualizations. Includes system resource monitoring and chunked processing.",
      "purpose": "To provide automated project analysis and visualization of file dependencies, relationships, and execution flow using AI assistance and network graphs",
      "execution_order": 75,
      "is_executable": true,
      "is_standalone": true,
      "calls": [
        {
          "file": "analysis_cache.json",
          "method": "source",
          "when": "startup",
          "line": 35,
          "critical": false
        },
        {
          "file": "dependency_map_ai.html",
          "method": "exec",
          "when": "runtime",
          "line": 36,
          "critical": false
        }
      ],
      "called_by_likely": [
        "main.py",
        "analyze.py",
        "project_scanner.py"
      ],
      "requires_before": [
        "pip install requirements",
        "ANTHROPIC_API_KEY setup"
      ],
      "produces_after": [
        "dependency_map_ai.html",
        "analysis_cache.json"
      ],
      "category": "monitor",
      "inputs": [
        "ANTHROPIC_API_KEY environment variable",
        "/home/tor/Downloads/ids2 directory",
        "Python/shell/yaml files in project",
        "analysis_cache.json (optional)"
      ],
      "outputs": [
        "dependency_map_ai.html interactive visualization",
        "analysis_cache.json cache file",
        "Console logs with progress monitoring",
        "Opened browser window"
      ],
      "potential_bugs": [
        {
          "type": "hardcoded_path",
          "description": "Hardcoded API key and root directory path in Config dataclass",
          "severity": "high",
          "line": 35,
          "suggestion": "Remove hardcoded API key from source code and make root_dir configurable via CLI args",
          "file": "intelligent_analyzer.py"
        },
        {
          "type": "missing_dependency",
          "description": "Multiple external dependencies (networkx, anthropic, pyvis, psutil) with basic import error handling but no version checking",
          "severity": "medium",
          "line": 30,
          "suggestion": "Add version compatibility checks and more detailed installation instructions",
          "file": "intelligent_analyzer.py"
        },
        {
          "type": "no_error_handling",
          "description": "ResourceMonitor thread operations lack proper exception handling",
          "severity": "medium",
          "line": 180,
          "suggestion": "Add try-catch blocks around psutil calls and thread operations",
          "file": "intelligent_analyzer.py"
        },
        {
          "type": "race_condition",
          "description": "Multiple threads accessing shared _stats dictionary with locks but potential race conditions in monitor updates",
          "severity": "medium",
          "line": 170,
          "suggestion": "Use atomic operations or more granular locking for thread-safe statistics updates",
          "file": "intelligent_analyzer.py"
        },
        {
          "type": "missing_env",
          "description": "ANTHROPIC_API_KEY falls back to hardcoded value instead of failing gracefully",
          "severity": "critical",
          "line": 35,
          "suggestion": "Fail with clear error message if API key is not provided via environment variable",
          "file": "intelligent_analyzer.py"
        },
        {
          "type": "unreachable",
          "description": "File content is truncated, potentially leaving incomplete class definitions and unreachable code",
          "severity": "high",
          "line": 200,
          "suggestion": "Complete the ResourceMonitor._loop method implementation",
          "file": "intelligent_analyzer.py"
        }
      ],
      "fsm_state": {
        "state_name": "ANALYZING",
        "preconditions": [
          "ANTHROPIC_API_KEY environment variable set",
          "Target directory exists and contains analyzable files",
          "Required Python packages installed",
          "Sufficient system resources available"
        ],
        "postconditions": [
          "HTML dependency visualization generated",
          "Cache file updated with analysis results",
          "Browser opened with interactive graph",
          "System resources released"
        ],
        "can_fail": true,
        "on_failure": "abort",
        "timeout_seconds": 300
      }
    }
  },
  "setup.sh": {
    "hash": "009bcf23d4adadae9afb40542220b7fd",
    "data": {
      "role": "entry_point",
      "description": "Remote deployment script that packages and installs an IDS (Intrusion Detection System) dashboard on a Raspberry Pi via SSH. Prompts for credentials, transfers files, and executes installation scripts remotely.",
      "purpose": "Automate the deployment process of the IDS dashboard from a local development machine to a Raspberry Pi target device",
      "execution_order": 1,
      "is_executable": true,
      "is_standalone": true,
      "calls": [
        {
          "file": "depancecmd/*.sh",
          "method": "exec",
          "when": "runtime",
          "line": 68,
          "critical": true
        },
        {
          "file": "/tmp/ids-dashboard.tar.gz",
          "method": "call",
          "when": "runtime",
          "line": 48,
          "critical": true
        }
      ],
      "called_by_likely": [
        "developer/admin running deployment manually",
        "CI/CD pipeline",
        "deployment automation scripts"
      ],
      "requires_before": [
        "depancecmd/*.sh installation scripts must exist",
        "sshpass package installed",
        "SSH access to target Pi"
      ],
      "produces_after": [
        "Remote Pi configured with IDS dashboard",
        "All depancecmd/*.sh scripts executed on target"
      ],
      "category": "deploy",
      "inputs": [
        "PI_HOST (IP address)",
        "PI_USER (SSH username)",
        "PI_PASS (SSH password)",
        "SUDO_PASS (sudo password)",
        "REMOTE_DIR (installation directory)",
        "MIRROR_INTERFACE (network interface)",
        "depancecmd/*.sh scripts",
        "project source code"
      ],
      "outputs": [
        "ids-dashboard.tar.gz archive",
        "Remote directory structure on Pi",
        "Installed services on Pi",
        "Configured IDS dashboard"
      ],
      "potential_bugs": [
        {
          "type": "missing_dependency",
          "description": "Script only checks for sshpass but doesn't verify tar, ssh, scp commands exist",
          "severity": "medium",
          "line": 14,
          "suggestion": "Add checks for all required commands: tar, ssh, scp",
          "file": "setup.sh"
        },
        {
          "type": "no_error_handling",
          "description": "Passwords stored in plain text variables accessible via process list",
          "severity": "critical",
          "line": 20,
          "suggestion": "Use SSH keys instead of passwords, or ensure passwords are not exposed in process environment",
          "file": "setup.sh"
        },
        {
          "type": "hardcoded_path",
          "description": "Hardcoded /tmp path for archive may not exist on all systems",
          "severity": "low",
          "line": 33,
          "suggestion": "Use TMPDIR environment variable or verify /tmp exists",
          "file": "setup.sh"
        },
        {
          "type": "race_condition",
          "description": "Temporary archive file could be accessed by other users before cleanup",
          "severity": "high",
          "line": 33,
          "suggestion": "Set restrictive permissions on temporary file: chmod 600",
          "file": "setup.sh"
        },
        {
          "type": "missing_env",
          "description": "REMOTE_DIR, INSTALL_USER, MIRROR_INTERFACE variables passed to remote scripts but not validated",
          "severity": "medium",
          "line": 69,
          "suggestion": "Validate all variables before passing to remote scripts",
          "file": "setup.sh"
        },
        {
          "type": "no_error_handling",
          "description": "SSH connection failures not properly handled - script continues even if remote commands fail",
          "severity": "high",
          "line": 25,
          "suggestion": "Add connection testing before main operations",
          "file": "setup.sh"
        },
        {
          "type": "missing_dependency",
          "description": "No cleanup of temporary archive file if script fails or succeeds",
          "severity": "medium",
          "line": 33,
          "suggestion": "Add trap to cleanup temporary files on exit",
          "file": "setup.sh"
        },
        {
          "type": "hardcoded_path",
          "description": "Assumes depancecmd/*.sh files exist without verification",
          "severity": "high",
          "line": 64,
          "suggestion": "Check if depancecmd directory and .sh files exist before attempting to execute",
          "file": "setup.sh"
        },
        {
          "type": "no_error_handling",
          "description": "Script continues execution even when individual installation scripts fail",
          "severity": "medium",
          "line": 69,
          "suggestion": "Consider adding option to stop on first failure or implement retry mechanism",
          "file": "setup.sh"
        }
      ],
      "fsm_state": {
        "state_name": "DEPLOYING",
        "preconditions": [
          "sshpass installed locally",
          "SSH access to target Pi",
          "depancecmd/*.sh scripts exist",
          "Valid Pi credentials provided"
        ],
        "postconditions": [
          "IDS dashboard deployed on Pi",
          "All installation scripts executed",
          "Remote services configured"
        ],
        "can_fail": true,
        "on_failure": "abort",
        "timeout_seconds": 300
      }
    }
  },
  "yu.py": {
    "hash": "30f11977225d06f3f240facd376c2846",
    "data": {
      "role": "utility",
      "description": "A dependency analyzer tool that scans project files to extract and visualize dependencies between different file types (Python, shell scripts, Dockerfiles, YAML configs, etc.) using NetworkX and Pyvis for interactive HTML visualization.",
      "purpose": "To provide project structure analysis and dependency mapping for debugging, documentation, and understanding complex project relationships through visual network graphs.",
      "execution_order": 85,
      "is_executable": true,
      "is_standalone": true,
      "calls": [
        {
          "file": "current_directory",
          "method": "source",
          "when": "startup",
          "line": 93,
          "critical": true
        },
        {
          "file": "dependency_map.html",
          "method": "exec",
          "when": "runtime",
          "line": 86,
          "critical": false
        }
      ],
      "called_by_likely": [
        "main.py",
        "analyze.py",
        "debug_scripts",
        "project_analysis_tools"
      ],
      "requires_before": [
        "project_files_to_analyze"
      ],
      "produces_after": [
        "dependency_map.html",
        "project_documentation"
      ],
      "category": "util",
      "inputs": [
        "project_directory_structure",
        "python_files",
        "shell_scripts",
        "dockerfiles",
        "yaml_configs",
        "service_files"
      ],
      "outputs": [
        "dependency_map.html",
        "console_analysis_output",
        "networkx_graph_object"
      ],
      "potential_bugs": [
        {
          "type": "no_error_handling",
          "description": "os.walk() and file operations lack comprehensive error handling for permission denied, corrupted files, or binary files",
          "severity": "medium",
          "line": 34,
          "suggestion": "Add try-catch around os.walk() and file reading operations with specific exception handling",
          "file": "yu.py"
        },
        {
          "type": "hardcoded_path",
          "description": "Hardcoded current directory '.' as default parameter may not work in all execution contexts",
          "severity": "low",
          "line": 93,
          "suggestion": "Use os.getcwd() or require explicit path parameter",
          "file": "yu.py"
        },
        {
          "type": "missing_dependency",
          "description": "No validation that required packages (networkx, pyvis) are installed before execution",
          "severity": "high",
          "line": 4,
          "suggestion": "Add import error handling with user-friendly installation instructions",
          "file": "yu.py"
        },
        {
          "type": "unreachable",
          "description": "Pattern matching may fail silently on malformed files, leading to incomplete dependency graphs",
          "severity": "medium",
          "line": 42,
          "suggestion": "Add logging for failed pattern matches and file parsing issues",
          "file": "yu.py"
        },
        {
          "type": "missing_env",
          "description": "No validation of write permissions for output HTML file creation",
          "severity": "medium",
          "line": 86,
          "suggestion": "Check write permissions and disk space before attempting file creation",
          "file": "yu.py"
        },
        {
          "type": "race_condition",
          "description": "Files could be modified during analysis leading to inconsistent results",
          "severity": "low",
          "line": 34,
          "suggestion": "Add file modification time tracking or atomic read operations",
          "file": "yu.py"
        }
      ],
      "fsm_state": {
        "state_name": "ANALYZING",
        "preconditions": [
          "project_files_exist",
          "python_dependencies_installed",
          "write_permissions_available"
        ],
        "postconditions": [
          "dependency_graph_created",
          "html_visualization_generated",
          "analysis_complete"
        ],
        "can_fail": true,
        "on_failure": "abort",
        "timeout_seconds": 300
      }
    }
  },
  "setup_venv.sh": {
    "hash": "6244bd98190f2547fda4b0a3fd8394a0",
    "data": {
      "role": "utility",
      "description": "Sets up a Python virtual environment named 'venv', installs dependencies from requirements.txt if present, and updates .gitignore to exclude the virtual environment.",
      "purpose": "Automates the initial Python project environment setup for the ids2 project to ensure isolated dependencies and consistent development environment",
      "execution_order": 5,
      "is_executable": true,
      "is_standalone": true,
      "calls": [
        {
          "file": "requirements.txt",
          "method": "exec",
          "when": "conditional",
          "line": 19,
          "critical": false
        },
        {
          "file": ".gitignore",
          "method": "exec",
          "when": "runtime",
          "line": 25,
          "critical": false
        }
      ],
      "called_by_likely": [
        "developer manual execution",
        "install.sh",
        "bootstrap.sh",
        "setup.sh"
      ],
      "requires_before": [
        "python3 installation",
        "python3-venv package"
      ],
      "produces_after": [
        "main.py",
        "any Python scripts in project",
        "pip install commands"
      ],
      "category": "setup",
      "inputs": [
        "requirements.txt (optional)",
        ".gitignore file",
        "python3 binary",
        "/home/tor/Downloads/ids2 directory"
      ],
      "outputs": [
        "venv/ directory with virtual environment",
        "installed Python packages",
        "updated .gitignore file"
      ],
      "potential_bugs": [
        {
          "type": "hardcoded_path",
          "description": "Script hardcodes path '/home/tor/Downloads/ids2' which won't exist on other systems",
          "severity": "critical",
          "line": 6,
          "suggestion": "Use relative paths or script directory detection like 'cd \"$(dirname \"$0\")\"'",
          "file": "setup_venv.sh"
        },
        {
          "type": "no_error_handling",
          "description": "No 'set -e' to exit on errors, script continues even if cd fails or venv creation fails",
          "severity": "high",
          "line": 1,
          "suggestion": "Add 'set -e' at top and check command exit codes",
          "file": "setup_venv.sh"
        },
        {
          "type": "missing_dependency",
          "description": "python3 and python3-venv availability not checked before use",
          "severity": "high",
          "line": 9,
          "suggestion": "Add dependency checks: 'command -v python3 >/dev/null || { echo \"python3 not found\"; exit 1; }'",
          "file": "setup_venv.sh"
        },
        {
          "type": "race_condition",
          "description": "No check if venv directory already exists, could overwrite existing environment",
          "severity": "medium",
          "line": 9,
          "suggestion": "Check if venv exists: 'if [ ! -d \"venv\" ]; then python3 -m venv venv; fi'",
          "file": "setup_venv.sh"
        },
        {
          "type": "missing_env",
          "description": "Script assumes specific directory structure and user permissions without validation",
          "severity": "medium",
          "line": 6,
          "suggestion": "Add directory existence and write permission checks",
          "file": "setup_venv.sh"
        },
        {
          "type": "unreachable",
          "description": "Sort command on .gitignore may fail if file doesn't exist initially",
          "severity": "low",
          "line": 26,
          "suggestion": "Ensure .gitignore exists: 'touch .gitignore' before sorting",
          "file": "setup_venv.sh"
        }
      ],
      "fsm_state": {
        "state_name": "INSTALLING",
        "preconditions": [
          "python3 installed",
          "python3-venv package available",
          "write permissions in target directory"
        ],
        "postconditions": [
          "virtual environment created",
          "dependencies installed",
          ".gitignore updated"
        ],
        "can_fail": true,
        "on_failure": "abort",
        "timeout_seconds": 300
      }
    }
  },
  "kl.py": {
    "hash": "0a22f90c944d9c42e110e6d50f79eba3",
    "data": {
      "role": "utility",
      "description": "Analyzes Python source files to identify external dependencies and suggests content for requirements.txt file.",
      "purpose": "Helps developers identify and manage external Python dependencies by scanning project files for import statements",
      "execution_order": 15,
      "is_executable": true,
      "is_standalone": true,
      "calls": [
        {
          "file": "all Python files in project",
          "method": "import",
          "when": "runtime",
          "line": 44,
          "critical": false
        }
      ],
      "called_by_likely": [
        "setup.py",
        "build scripts",
        "deployment scripts",
        "developers manually"
      ],
      "requires_before": [
        "Python project files must exist"
      ],
      "produces_after": [
        "requirements.txt creation",
        "dependency installation scripts"
      ],
      "category": "util",
      "inputs": [
        "Python source files in project directory",
        "PROJECT_DIR path",
        "file system access"
      ],
      "outputs": [
        "List of external dependencies",
        "requirements.txt suggestions",
        "Console output with analysis results"
      ],
      "potential_bugs": [
        {
          "type": "no_error_handling",
          "description": "File reading errors are caught but script continues without proper error reporting",
          "severity": "medium",
          "line": 43,
          "suggestion": "Add proper logging and error aggregation",
          "file": "kl.py"
        },
        {
          "type": "hardcoded_path",
          "description": "PROJECT_DIR hardcoded to current directory, may not work in all contexts",
          "severity": "low",
          "line": 6,
          "suggestion": "Accept PROJECT_DIR as command line argument or environment variable",
          "file": "kl.py"
        },
        {
          "type": "dead_code",
          "description": "Double negation 'not not match.startswith()' makes conditions always true",
          "severity": "critical",
          "line": 32,
          "suggestion": "Remove 'not not' - should be just 'not match.startswith()'",
          "file": "kl.py"
        },
        {
          "type": "missing_dependency",
          "description": "Hardcoded exclusion list may miss legitimate dependencies or include false positives",
          "severity": "medium",
          "line": 30,
          "suggestion": "Use ast module for proper Python parsing instead of regex",
          "file": "kl.py"
        },
        {
          "type": "unreachable",
          "description": "Logic error in filtering - double negation makes filters ineffective",
          "severity": "high",
          "line": 32,
          "suggestion": "Fix boolean logic: change 'not not match.startswith(ids)' to 'not match.startswith(ids)'",
          "file": "kl.py"
        },
        {
          "type": "missing_error_handling",
          "description": "os.walk and file operations could fail with permission errors",
          "severity": "medium",
          "line": 10,
          "suggestion": "Add try-catch around os.walk and handle permission errors gracefully",
          "file": "kl.py"
        }
      ],
      "fsm_state": {
        "state_name": "ANALYZING_DEPENDENCIES",
        "preconditions": [
          "Python project exists",
          "File system is readable",
          "Current directory contains Python files"
        ],
        "postconditions": [
          "External dependencies identified",
          "Requirements suggestions generated",
          "Analysis report displayed"
        ],
        "can_fail": true,
        "on_failure": "continue with partial results",
        "timeout_seconds": 30
      }
    }
  },
  "00-check-apt-sources.sh": {
    "hash": "f4892b2fe1abf36e4cadc86afa3b7cbb",
    "data": {
      "role": "utility",
      "description": "Validates that APT package manager sources are properly configured by checking for existence of sources.list files or sources.list.d directory entries.",
      "purpose": "Pre-flight check to ensure APT package sources are available before attempting package operations",
      "execution_order": 5,
      "is_executable": true,
      "is_standalone": true,
      "calls": [
        {
          "file": "/etc/apt/sources.list.d",
          "method": "filesystem",
          "when": "startup",
          "line": 4,
          "critical": true
        },
        {
          "file": "/etc/apt/sources.list",
          "method": "filesystem",
          "when": "startup",
          "line": 4,
          "critical": true
        },
        {
          "file": "/etc/apt/sources.list.d/*.list",
          "method": "filesystem",
          "when": "runtime",
          "line": 9,
          "critical": false
        },
        {
          "file": "/etc/apt/sources.list.d/*.sources",
          "method": "filesystem",
          "when": "runtime",
          "line": 9,
          "critical": false
        }
      ],
      "called_by_likely": [
        "install.sh",
        "setup.sh",
        "package-installer.sh",
        "system-setup.sh"
      ],
      "requires_before": [],
      "produces_after": [
        "apt-get",
        "apt",
        "package installation scripts"
      ],
      "category": "setup",
      "inputs": [
        "/etc/apt/sources.list",
        "/etc/apt/sources.list.d/",
        "APT configuration files"
      ],
      "outputs": [
        "console messages in French",
        "exit status (0=success, 1=failure)"
      ],
      "potential_bugs": [
        {
          "type": "hardcoded_path",
          "description": "Script assumes standard Debian/Ubuntu APT paths which may not exist on other distributions or containerized environments",
          "severity": "medium",
          "line": 4,
          "suggestion": "Add distribution detection or allow path override via environment variables",
          "file": "00-check-apt-sources.sh"
        },
        {
          "type": "race_condition",
          "description": "Time-of-check-time-of-use issue between checking file existence and later APT operations",
          "severity": "low",
          "line": 9,
          "suggestion": "This is acceptable for a pre-flight check as APT will handle missing sources gracefully",
          "file": "00-check-apt-sources.sh"
        },
        {
          "type": "missing_dependency",
          "description": "Script doesn't verify if 'ls' command exists, though this is extremely unlikely to be missing",
          "severity": "low",
          "line": 9,
          "suggestion": "Use built-in bash test operators instead of ls for better portability",
          "file": "00-check-apt-sources.sh"
        },
        {
          "type": "no_error_handling",
          "description": "While set -euo pipefail is used, there's no cleanup or specific error context when checks fail",
          "severity": "low",
          "line": 1,
          "suggestion": "Consider adding more descriptive error messages indicating which specific check failed",
          "file": "00-check-apt-sources.sh"
        }
      ],
      "fsm_state": {
        "state_name": "VALIDATING_APT_SOURCES",
        "preconditions": [
          "system has APT package manager",
          "filesystem is readable"
        ],
        "postconditions": [
          "APT sources confirmed available",
          "safe to proceed with package operations"
        ],
        "can_fail": true,
        "on_failure": "abort",
        "timeout_seconds": 5
      }
    }
  },
  "09-configure-network.sh": {
    "hash": "fa728ed1a934b924423b0952938eed1a",
    "data": {
      "role": "config",
      "description": "Configures network interface for traffic mirroring by disabling all interfaces except the designated mirror interface and enabling promiscuous mode on it.",
      "purpose": "Prepares the system for network traffic monitoring/mirroring by isolating traffic to one interface and enabling packet capture capabilities",
      "execution_order": 9,
      "is_executable": true,
      "is_standalone": false,
      "calls": [
        {
          "file": "ip",
          "method": "subprocess",
          "when": "runtime",
          "line": 9,
          "critical": true
        },
        {
          "file": "awk",
          "method": "subprocess",
          "when": "runtime",
          "line": 9,
          "critical": true
        },
        {
          "file": "ip",
          "method": "subprocess",
          "when": "runtime",
          "line": 7,
          "critical": true
        },
        {
          "file": "ip",
          "method": "subprocess",
          "when": "runtime",
          "line": 12,
          "critical": true
        }
      ],
      "called_by_likely": [
        "01-main-install.sh",
        "setup.sh",
        "deployment-script.sh"
      ],
      "requires_before": [
        "01-install-dependencies.sh",
        "02-configure-system.sh"
      ],
      "produces_after": [
        "10-start-services.sh",
        "packet-capture-scripts.sh"
      ],
      "category": "config",
      "inputs": [
        "MIRROR_INTERFACE environment variable (defaults to eth0)",
        "system network interfaces",
        "root privileges"
      ],
      "outputs": [
        "disabled non-mirror network interfaces",
        "promiscuous mode enabled on mirror interface",
        "system ready for network monitoring"
      ],
      "potential_bugs": [
        {
          "type": "missing_dependency",
          "description": "Script assumes 'ip' command is available but doesn't verify it exists",
          "severity": "high",
          "line": 7,
          "suggestion": "Add 'command -v ip >/dev/null || { echo \"ip command not found\"; exit 1; }'",
          "file": "09-configure-network.sh"
        },
        {
          "type": "hardcoded_path",
          "description": "Default interface 'eth0' may not exist on all systems",
          "severity": "medium",
          "line": 3,
          "suggestion": "Auto-detect available interfaces or validate MIRROR_INTERFACE exists",
          "file": "09-configure-network.sh"
        },
        {
          "type": "no_error_handling",
          "description": "Using '|| true' suppresses all errors which could hide critical failures",
          "severity": "medium",
          "line": 7,
          "suggestion": "Check specific error conditions instead of blindly suppressing with || true",
          "file": "09-configure-network.sh"
        },
        {
          "type": "race_condition",
          "description": "Disabling interfaces while system might be using them could cause network disruption",
          "severity": "high",
          "line": 6,
          "suggestion": "Add checks for active connections or add delay/confirmation",
          "file": "09-configure-network.sh"
        },
        {
          "type": "missing_env",
          "description": "No validation that MIRROR_INTERFACE is a valid interface name",
          "severity": "medium",
          "line": 3,
          "suggestion": "Add validation: ip link show \"$MIRROR_INTERFACE\" >/dev/null 2>&1",
          "file": "09-configure-network.sh"
        },
        {
          "type": "unreachable",
          "description": "If ip -o link show fails, the while loop gets no input but script continues",
          "severity": "low",
          "line": 9,
          "suggestion": "Verify ip command succeeds before processing output",
          "file": "09-configure-network.sh"
        }
      ],
      "fsm_state": {
        "state_name": "CONFIGURING_NETWORK",
        "preconditions": [
          "system has network interfaces",
          "script has root privileges",
          "ip command is available"
        ],
        "postconditions": [
          "only mirror interface and loopback are active",
          "mirror interface is in promiscuous mode",
          "system ready for packet capture"
        ],
        "can_fail": true,
        "on_failure": "abort",
        "timeout_seconds": 30
      }
    }
  },
  "05-install-docker.sh": {
    "hash": "dcc082a3ecf2721fe10f763eda13c563",
    "data": {
      "role": "utility",
      "description": "Installs Docker Engine via apt with GPG key verification and adds user to docker group. Enables Docker service and handles compose plugin installation.",
      "purpose": "Provides Docker installation as a dependency for containerized services in the deployment pipeline",
      "execution_order": 5,
      "is_executable": true,
      "is_standalone": true,
      "calls": [
        {
          "file": "/etc/os-release",
          "method": "source",
          "when": "runtime",
          "line": 34,
          "critical": false
        },
        {
          "file": "/etc/apt/keyrings/docker.gpg",
          "method": "exec",
          "when": "runtime",
          "line": 39,
          "critical": true
        },
        {
          "file": "/etc/apt/sources.list.d/docker.list",
          "method": "exec",
          "when": "runtime",
          "line": 51,
          "critical": true
        }
      ],
      "called_by_likely": [
        "00-main-install.sh",
        "setup.sh",
        "install-all.sh"
      ],
      "requires_before": [
        "apt-get update capability",
        "internet connectivity",
        "root privileges"
      ],
      "produces_after": [
        "docker daemon running",
        "user in docker group",
        "docker compose available"
      ],
      "category": "install",
      "inputs": [
        "INSTALL_USER env var",
        "SUDO_USER env var",
        "USER env var",
        "internet access",
        "apt package manager",
        "/etc/os-release file"
      ],
      "outputs": [
        "docker service enabled",
        "docker group membership",
        "Docker Engine installed",
        "docker-compose plugin installed",
        "/etc/apt/keyrings/docker.gpg",
        "/etc/apt/sources.list.d/docker.list"
      ],
      "potential_bugs": [
        {
          "type": "missing_error_handling",
          "description": "curl command could fail without proper error handling despite set -e",
          "severity": "medium",
          "line": 39,
          "suggestion": "Add explicit error checking after curl command",
          "file": "05-install-docker.sh"
        },
        {
          "type": "hardcoded_path",
          "description": "Hardcoded fallback codename 'bookworm' may not work on all Debian derivatives",
          "severity": "medium",
          "line": 47,
          "suggestion": "Use dynamic detection or support multiple fallback codenames",
          "file": "05-install-docker.sh"
        },
        {
          "type": "race_condition",
          "description": "Multiple apt-get update calls could conflict if run in parallel",
          "severity": "low",
          "line": 26,
          "suggestion": "Add flock around apt operations",
          "file": "05-install-docker.sh"
        },
        {
          "type": "missing_dependency",
          "description": "Script assumes systemctl exists but only checks at runtime",
          "severity": "low",
          "line": 19,
          "suggestion": "Add fallback for non-systemd systems",
          "file": "05-install-docker.sh"
        },
        {
          "type": "no_error_handling",
          "description": "usermod command uses || true which silently ignores failures",
          "severity": "medium",
          "line": 16,
          "suggestion": "Check if usermod actually succeeded and warn user",
          "file": "05-install-docker.sh"
        },
        {
          "type": "missing_env",
          "description": "If all user variables are empty, docker group setup silently skips",
          "severity": "medium",
          "line": 4,
          "suggestion": "Warn user when no valid user found for docker group",
          "file": "05-install-docker.sh"
        },
        {
          "type": "hardcoded_path",
          "description": "GPG keyring path /etc/apt/keyrings may not exist on older systems",
          "severity": "medium",
          "line": 28,
          "suggestion": "Check if directory exists before install command",
          "file": "05-install-docker.sh"
        },
        {
          "type": "missing_error_handling",
          "description": "No verification that docker service actually started after enable",
          "severity": "medium",
          "line": 20,
          "suggestion": "Add docker service status check after enable",
          "file": "05-install-docker.sh"
        }
      ],
      "fsm_state": {
        "state_name": "INSTALLING_DOCKER",
        "preconditions": [
          "root privileges available",
          "apt package manager working",
          "internet connectivity",
          "system is Debian-based"
        ],
        "postconditions": [
          "docker daemon running",
          "docker command available",
          "user in docker group",
          "docker compose plugin installed"
        ],
        "can_fail": true,
        "on_failure": "abort",
        "timeout_seconds": 300
      }
    }
  },
  "01-update-upgrade.sh": {
    "hash": "cdfa91ac97881e9288b29d423ab5cbf3",
    "data": {
      "role": "utility",
      "description": "Updates APT package lists and upgrades all installed Debian/Ubuntu packages to their latest versions in non-interactive mode.",
      "purpose": "System maintenance script to ensure all packages are up-to-date before installing new dependencies or running applications",
      "execution_order": 5,
      "is_executable": true,
      "is_standalone": true,
      "calls": [
        {
          "file": "apt-get",
          "method": "exec",
          "when": "runtime",
          "line": 6,
          "critical": true
        },
        {
          "file": "apt-get",
          "method": "exec",
          "when": "runtime",
          "line": 7,
          "critical": true
        }
      ],
      "called_by_likely": [
        "install.sh",
        "setup.sh",
        "bootstrap.sh",
        "Dockerfile",
        "deployment scripts"
      ],
      "requires_before": [
        "network connectivity",
        "root privileges check"
      ],
      "produces_after": [
        "package installation scripts",
        "dependency installation scripts"
      ],
      "category": "setup",
      "inputs": [
        "root/sudo privileges",
        "internet connection",
        "APT package manager",
        "/var/lib/apt/lists/"
      ],
      "outputs": [
        "updated package lists in /var/lib/apt/lists/",
        "upgraded system packages",
        "updated package cache"
      ],
      "potential_bugs": [
        {
          "type": "missing_dependency",
          "description": "No check for root/sudo privileges before running apt-get commands",
          "severity": "high",
          "line": 6,
          "suggestion": "Add privilege check: if [[ $EUID -ne 0 ]]; then echo 'Root required'; exit 1; fi",
          "file": "01-update-upgrade.sh"
        },
        {
          "type": "no_error_handling",
          "description": "No specific error handling for network issues or package conflicts during upgrade",
          "severity": "medium",
          "line": 6,
          "suggestion": "Add retry logic and specific error messages for common apt-get failures",
          "file": "01-update-upgrade.sh"
        },
        {
          "type": "missing_dependency",
          "description": "No verification that apt-get exists (could fail on non-Debian systems)",
          "severity": "medium",
          "line": 6,
          "suggestion": "Add check: command -v apt-get >/dev/null 2>&1 || { echo 'apt-get not found'; exit 1; }",
          "file": "01-update-upgrade.sh"
        },
        {
          "type": "race_condition",
          "description": "Could conflict with other apt processes if run in parallel",
          "severity": "medium",
          "line": 6,
          "suggestion": "Add apt lock check or use flock to prevent concurrent apt operations",
          "file": "01-update-upgrade.sh"
        },
        {
          "type": "missing_dependency",
          "description": "No disk space check before upgrade which could fill filesystem",
          "severity": "low",
          "line": 7,
          "suggestion": "Check available space in /var/cache/apt/archives before upgrade",
          "file": "01-update-upgrade.sh"
        }
      ],
      "fsm_state": {
        "state_name": "UPDATING_SYSTEM",
        "preconditions": [
          "root privileges available",
          "network connectivity",
          "APT package manager present",
          "no other apt processes running"
        ],
        "postconditions": [
          "package lists updated",
          "all packages upgraded to latest versions",
          "system ready for new package installations"
        ],
        "can_fail": true,
        "on_failure": "abort",
        "timeout_seconds": 1800
      }
    }
  },
  "06-install-suricata.sh": {
    "hash": "8c69edb20347db2ddbc6e880f7a0e81e",
    "data": {
      "role": "utility",
      "description": "Installs Suricata network intrusion detection system and updates its rule sets on Debian/Ubuntu systems.",
      "purpose": "Sets up network security monitoring capabilities by installing Suricata IDS/IPS and ensuring it has current threat detection rules",
      "execution_order": 6,
      "is_executable": true,
      "is_standalone": true,
      "calls": [
        {
          "file": "apt-get",
          "method": "exec",
          "when": "runtime",
          "line": 6,
          "critical": true
        },
        {
          "file": "suricata-update",
          "method": "exec",
          "when": "conditional",
          "line": 9,
          "critical": false
        }
      ],
      "called_by_likely": [
        "main-installer.sh",
        "security-setup.sh",
        "00-master-installer.sh"
      ],
      "requires_before": [
        "package manager update scripts",
        "network connectivity setup"
      ],
      "produces_after": [
        "suricata configuration scripts",
        "network monitoring scripts"
      ],
      "category": "install",
      "inputs": [
        "root privileges",
        "internet connectivity",
        "apt package manager",
        "debian/ubuntu system"
      ],
      "outputs": [
        "suricata binary installed",
        "suricata-update tool",
        "updated threat detection rules",
        "/etc/suricata/ config directory"
      ],
      "potential_bugs": [
        {
          "type": "missing_dependency",
          "description": "No check if apt-get is available or if system is debian-based",
          "severity": "high",
          "line": 6,
          "suggestion": "Add check for apt-get availability and OS detection",
          "file": "06-install-suricata.sh"
        },
        {
          "type": "no_error_handling",
          "description": "apt-get install could fail due to network issues, package conflicts, or insufficient disk space",
          "severity": "medium",
          "line": 6,
          "suggestion": "Add retry logic and better error messages for apt-get failures",
          "file": "06-install-suricata.sh"
        },
        {
          "type": "missing_dependency",
          "description": "No check for internet connectivity before suricata-update",
          "severity": "medium",
          "line": 9,
          "suggestion": "Add network connectivity check before attempting rule updates",
          "file": "06-install-suricata.sh"
        },
        {
          "type": "hardcoded_path",
          "description": "Assumes standard apt-get location and debian package names",
          "severity": "medium",
          "line": 6,
          "suggestion": "Add OS detection and use appropriate package manager",
          "file": "06-install-suricata.sh"
        },
        {
          "type": "race_condition",
          "description": "If multiple instances run simultaneously, apt locks could cause failures",
          "severity": "low",
          "line": 6,
          "suggestion": "Add apt lock checking and waiting mechanism",
          "file": "06-install-suricata.sh"
        },
        {
          "type": "missing_env",
          "description": "No validation that DEBIAN_FRONTEND export is working correctly",
          "severity": "low",
          "line": 4,
          "suggestion": "Verify environment variable is set correctly",
          "file": "06-install-suricata.sh"
        }
      ],
      "fsm_state": {
        "state_name": "INSTALLING_SECURITY_TOOLS",
        "preconditions": [
          "system has root privileges",
          "apt package manager available",
          "internet connectivity"
        ],
        "postconditions": [
          "suricata installed and functional",
          "threat detection rules updated",
          "system ready for network monitoring"
        ],
        "can_fail": true,
        "on_failure": "abort",
        "timeout_seconds": 300
      }
    }
  },
  "02-install-base.sh": {
    "hash": "afcc9fdd6ee85c190b19998b622135db",
    "data": {
      "role": "utility",
      "description": "Installs base system packages required for the project using apt-get with non-interactive mode.",
      "purpose": "Sets up fundamental dependencies and build tools needed before other installation scripts can run",
      "execution_order": 2,
      "is_executable": true,
      "is_standalone": true,
      "calls": [
        {
          "file": "apt-get",
          "method": "exec",
          "when": "runtime",
          "line": 6,
          "critical": true
        }
      ],
      "called_by_likely": [
        "01-setup.sh",
        "main-install.sh",
        "bootstrap.sh"
      ],
      "requires_before": [
        "01-*"
      ],
      "produces_after": [
        "03-*",
        "docker installation scripts",
        "build scripts"
      ],
      "category": "install",
      "inputs": [
        "root/sudo privileges",
        "internet connection",
        "debian/ubuntu system",
        "apt package manager"
      ],
      "outputs": [
        "ca-certificates",
        "curl",
        "gnupg",
        "lsb-release",
        "build-essential",
        "jq"
      ],
      "potential_bugs": [
        {
          "type": "missing_dependency",
          "description": "No check if apt-get is available or system is debian-based",
          "severity": "high",
          "line": 6,
          "suggestion": "Add check for apt-get existence and OS compatibility",
          "file": "02-install-base.sh"
        },
        {
          "type": "no_error_handling",
          "description": "No verification that packages were actually installed successfully",
          "severity": "medium",
          "line": 6,
          "suggestion": "Add post-install verification commands",
          "file": "02-install-base.sh"
        },
        {
          "type": "missing_dependency",
          "description": "No check for sudo/root privileges before attempting system package installation",
          "severity": "critical",
          "line": 6,
          "suggestion": "Add privilege check at start of script",
          "file": "02-install-base.sh"
        },
        {
          "type": "missing_dependency",
          "description": "No network connectivity check before attempting downloads",
          "severity": "medium",
          "line": 6,
          "suggestion": "Add network connectivity test",
          "file": "02-install-base.sh"
        },
        {
          "type": "race_condition",
          "description": "apt-get update not called, could install stale packages if run in parallel with other apt operations",
          "severity": "medium",
          "line": 6,
          "suggestion": "Add 'apt-get update' before install or use apt-get update && apt-get install",
          "file": "02-install-base.sh"
        },
        {
          "type": "missing_env",
          "description": "DEBIAN_FRONTEND set but not checked if it's already set to something important",
          "severity": "low",
          "line": 4,
          "suggestion": "Store and restore original DEBIAN_FRONTEND value",
          "file": "02-install-base.sh"
        }
      ],
      "fsm_state": {
        "state_name": "INSTALLING_BASE_PACKAGES",
        "preconditions": [
          "system has apt-get",
          "internet connection available",
          "root/sudo privileges",
          "debian/ubuntu OS"
        ],
        "postconditions": [
          "base build tools installed",
          "curl available",
          "certificate store updated",
          "jq available for JSON parsing"
        ],
        "can_fail": true,
        "on_failure": "abort",
        "timeout_seconds": 300
      }
    }
  },
  "08-install-sqlite.sh": {
    "hash": "fa860257d917cd5cdd603979726959a7",
    "data": {
      "role": "utility",
      "description": "Installs SQLite3 database engine using apt-get package manager on Debian/Ubuntu systems.",
      "purpose": "Provides SQLite3 database functionality as a dependency for the project, likely for local data storage or testing",
      "execution_order": 8,
      "is_executable": true,
      "is_standalone": true,
      "calls": [
        {
          "file": "apt-get",
          "method": "exec",
          "when": "runtime",
          "line": 5,
          "critical": true
        }
      ],
      "called_by_likely": [
        "main install script",
        "setup.sh",
        "bootstrap script",
        "00-install-*.sh"
      ],
      "requires_before": [
        "apt-get update script",
        "package manager initialization"
      ],
      "produces_after": [
        "scripts that use sqlite3",
        "database initialization scripts",
        "application startup scripts"
      ],
      "category": "install",
      "inputs": [
        "apt package manager",
        "internet connection",
        "root/sudo privileges",
        "Debian/Ubuntu system"
      ],
      "outputs": [
        "sqlite3 binary installed",
        "/usr/bin/sqlite3 executable",
        "SQLite3 system libraries"
      ],
      "potential_bugs": [
        {
          "type": "missing_dependency",
          "description": "No apt-get update before install, may fail if package lists are stale",
          "severity": "medium",
          "line": 5,
          "suggestion": "Run apt-get update before install or ensure it's run in a previous script",
          "file": "08-install-sqlite.sh"
        },
        {
          "type": "missing_env",
          "description": "Assumes Debian/Ubuntu system with apt-get, will fail on other distributions",
          "severity": "high",
          "line": 5,
          "suggestion": "Add OS detection or document system requirements",
          "file": "08-install-sqlite.sh"
        },
        {
          "type": "no_error_handling",
          "description": "No verification that sqlite3 was actually installed successfully",
          "severity": "medium",
          "line": 5,
          "suggestion": "Add post-install verification like 'sqlite3 --version'",
          "file": "08-install-sqlite.sh"
        },
        {
          "type": "hardcoded_path",
          "description": "Assumes apt-get is in PATH and available",
          "severity": "low",
          "line": 5,
          "suggestion": "Use full path /usr/bin/apt-get or check availability first",
          "file": "08-install-sqlite.sh"
        },
        {
          "type": "race_condition",
          "description": "Could conflict if multiple package installations run simultaneously",
          "severity": "medium",
          "line": 5,
          "suggestion": "Use package manager locking or run installs sequentially",
          "file": "08-install-sqlite.sh"
        }
      ],
      "fsm_state": {
        "state_name": "INSTALLING_SQLITE",
        "preconditions": [
          "apt package manager available",
          "internet connectivity",
          "root/sudo privileges",
          "package lists accessible"
        ],
        "postconditions": [
          "sqlite3 binary installed",
          "sqlite3 command available in PATH",
          "SQLite libraries available for linking"
        ],
        "can_fail": true,
        "on_failure": "abort",
        "timeout_seconds": 120
      }
    }
  },
  "10-install-backend-deps.sh": {
    "hash": "05ab66f6937e7424ef918f416d6619d5",
    "data": {
      "role": "utility",
      "description": "Installs Python backend dependencies in a virtual environment by creating a venv and installing packages from requirements.txt.",
      "purpose": "Ensures the backend Python application has all required dependencies installed in an isolated virtual environment",
      "execution_order": 15,
      "is_executable": true,
      "is_standalone": false,
      "calls": [
        {
          "file": "requirements.txt",
          "method": "exec",
          "when": "runtime",
          "line": 18,
          "critical": true
        },
        {
          "file": "python3",
          "method": "subprocess",
          "when": "conditional",
          "line": 14,
          "critical": true
        },
        {
          "file": "pip",
          "method": "subprocess",
          "when": "runtime",
          "line": 17,
          "critical": true
        },
        {
          "file": "pip",
          "method": "subprocess",
          "when": "runtime",
          "line": 18,
          "critical": true
        }
      ],
      "called_by_likely": [
        "main-install.sh",
        "setup.sh",
        "deploy.sh",
        "00-main-deployment-script.sh"
      ],
      "requires_before": [
        "repository clone",
        "python3 installation",
        "requirements.txt creation"
      ],
      "produces_after": [
        "backend application startup",
        "web server initialization",
        "API service launch"
      ],
      "category": "install",
      "inputs": [
        "REMOTE_DIR environment variable",
        "requirements.txt file",
        "python3 binary",
        "filesystem write permissions"
      ],
      "outputs": [
        "virtual environment directory",
        "installed Python packages",
        "upgraded pip",
        "venv activation scripts"
      ],
      "potential_bugs": [
        {
          "type": "missing_dependency",
          "description": "python3 command may not exist or be in PATH",
          "severity": "critical",
          "line": 14,
          "suggestion": "Add check for python3 availability before using",
          "file": "10-install-backend-deps.sh"
        },
        {
          "type": "missing_dependency",
          "description": "python3-venv package may not be installed on system",
          "severity": "critical",
          "line": 14,
          "suggestion": "Install python3-venv package or check for module availability",
          "file": "10-install-backend-deps.sh"
        },
        {
          "type": "hardcoded_path",
          "description": "Assumes /opt/ids-dashboard structure exists",
          "severity": "high",
          "line": 4,
          "suggestion": "Verify parent directories exist before proceeding",
          "file": "10-install-backend-deps.sh"
        },
        {
          "type": "no_error_handling",
          "description": "Network failures during pip install are not handled gracefully",
          "severity": "medium",
          "line": 18,
          "suggestion": "Add retry logic or offline fallback for pip install",
          "file": "10-install-backend-deps.sh"
        },
        {
          "type": "missing_env",
          "description": "If REMOTE_DIR is empty string, paths become invalid",
          "severity": "medium",
          "line": 4,
          "suggestion": "Validate REMOTE_DIR is not empty before using",
          "file": "10-install-backend-deps.sh"
        },
        {
          "type": "race_condition",
          "description": "Multiple instances could try to create venv simultaneously",
          "severity": "low",
          "line": 14,
          "suggestion": "Use file locking or atomic directory creation",
          "file": "10-install-backend-deps.sh"
        },
        {
          "type": "missing_dependency",
          "description": "Requirements file could contain packages that fail to install",
          "severity": "medium",
          "line": 18,
          "suggestion": "Parse requirements and validate each package individually",
          "file": "10-install-backend-deps.sh"
        }
      ],
      "fsm_state": {
        "state_name": "INSTALLING_BACKEND_DEPS",
        "preconditions": [
          "python3 available",
          "requirements.txt exists",
          "filesystem writable",
          "internet connectivity"
        ],
        "postconditions": [
          "virtual environment created",
          "pip upgraded",
          "all requirements installed",
          "backend ready for startup"
        ],
        "can_fail": true,
        "on_failure": "abort",
        "timeout_seconds": 300
      }
    }
  },
  "03-install-python.sh": {
    "hash": "09e3479ce2e4c58e886fdde285bb534e",
    "data": {
      "role": "utility",
      "description": "Installs Python 3 and related development packages on a Debian/Ubuntu system using apt-get.",
      "purpose": "Sets up Python runtime environment and development tools as a dependency for the project",
      "execution_order": 15,
      "is_executable": true,
      "is_standalone": true,
      "calls": [
        {
          "file": "apt-get",
          "method": "exec",
          "when": "runtime",
          "line": 6,
          "critical": true
        }
      ],
      "called_by_likely": [
        "main install script",
        "bootstrap script",
        "dockerfile",
        "CI/CD pipeline"
      ],
      "requires_before": [
        "package manager update (apt-get update)",
        "system initialization"
      ],
      "produces_after": [
        "Python application installs",
        "pip package installs",
        "virtual environment creation"
      ],
      "category": "install",
      "inputs": [
        "Debian/Ubuntu system",
        "root/sudo privileges",
        "internet connectivity",
        "apt package manager"
      ],
      "outputs": [
        "python3 binary",
        "python3-venv module",
        "python3-pip package manager",
        "python3-dev headers"
      ],
      "potential_bugs": [
        {
          "type": "missing_dependency",
          "description": "No apt-get update before install - packages might be outdated or missing",
          "severity": "medium",
          "line": 6,
          "suggestion": "Run 'apt-get update' before installing packages",
          "file": "03-install-python.sh"
        },
        {
          "type": "hardcoded_path",
          "description": "Assumes apt-get is available and system is Debian/Ubuntu based",
          "severity": "medium",
          "line": 6,
          "suggestion": "Add OS detection or document system requirements",
          "file": "03-install-python.sh"
        },
        {
          "type": "no_error_handling",
          "description": "No verification that packages were actually installed successfully",
          "severity": "low",
          "line": 6,
          "suggestion": "Add post-install verification like 'python3 --version'",
          "file": "03-install-python.sh"
        },
        {
          "type": "missing_dependency",
          "description": "Script assumes internet connectivity without checking",
          "severity": "medium",
          "line": 6,
          "suggestion": "Add network connectivity check before attempting downloads",
          "file": "03-install-python.sh"
        },
        {
          "type": "missing_env",
          "description": "No check for sufficient privileges to install system packages",
          "severity": "high",
          "line": 6,
          "suggestion": "Add check for root/sudo privileges before attempting apt-get install",
          "file": "03-install-python.sh"
        }
      ],
      "fsm_state": {
        "state_name": "INSTALLING_PYTHON",
        "preconditions": [
          "Debian/Ubuntu system available",
          "apt package manager functional",
          "internet connectivity",
          "root/sudo privileges"
        ],
        "postconditions": [
          "Python 3 runtime installed",
          "pip package manager available",
          "virtual environment support ready",
          "Python development headers installed"
        ],
        "can_fail": true,
        "on_failure": "abort",
        "timeout_seconds": 300
      }
    }
  },
  "04-install-node.sh": {
    "hash": "d8744cb3510c63b6633ce59c6136335f",
    "data": {
      "role": "entry_point",
      "description": "Installs Node.js and npm packages on a Debian-based system using apt-get package manager.",
      "purpose": "To set up Node.js runtime environment as part of a larger dependency installation sequence",
      "execution_order": 15,
      "is_executable": true,
      "is_standalone": true,
      "calls": [
        {
          "file": "apt-get",
          "method": "exec",
          "when": "runtime",
          "line": 6,
          "critical": true
        }
      ],
      "called_by_likely": [
        "00-main-install.sh",
        "install.sh",
        "setup.sh",
        "bootstrap.sh"
      ],
      "requires_before": [
        "01-update-system.sh",
        "02-install-basics.sh",
        "03-setup-repos.sh"
      ],
      "produces_after": [
        "05-install-python-deps.sh",
        "06-setup-nodejs-apps.sh",
        "npm-based-installs.sh"
      ],
      "category": "install",
      "inputs": [
        "root/sudo privileges",
        "internet connection",
        "debian-based OS",
        "apt package manager"
      ],
      "outputs": [
        "nodejs binary",
        "npm binary",
        "/usr/bin/node",
        "/usr/bin/npm",
        "system package database updated"
      ],
      "potential_bugs": [
        {
          "type": "missing_dependency",
          "description": "apt-get update not called before install, could install outdated packages",
          "severity": "medium",
          "line": 6,
          "suggestion": "Run apt-get update first or ensure it was run recently",
          "file": "04-install-node.sh"
        },
        {
          "type": "no_error_handling",
          "description": "No specific error handling for network failures during package download",
          "severity": "low",
          "line": 6,
          "suggestion": "Add retry logic or better error messages for network issues",
          "file": "04-install-node.sh"
        },
        {
          "type": "hardcoded_path",
          "description": "Assumes apt-get package manager exists (Debian/Ubuntu only)",
          "severity": "medium",
          "line": 6,
          "suggestion": "Add OS detection or document OS requirements",
          "file": "04-install-node.sh"
        },
        {
          "type": "missing_dependency",
          "description": "No check if packages are already installed, causing unnecessary reinstalls",
          "severity": "low",
          "line": 6,
          "suggestion": "Add package existence check before installation",
          "file": "04-install-node.sh"
        },
        {
          "type": "missing_env",
          "description": "Relies on PATH containing apt-get but doesn't verify",
          "severity": "low",
          "line": 6,
          "suggestion": "Use full path /usr/bin/apt-get or verify command exists",
          "file": "04-install-node.sh"
        }
      ],
      "fsm_state": {
        "state_name": "INSTALLING_NODEJS_RUNTIME",
        "preconditions": [
          "system updated",
          "apt package manager available",
          "root privileges",
          "network connectivity"
        ],
        "postconditions": [
          "nodejs installed",
          "npm installed",
          "node runtime available globally"
        ],
        "can_fail": true,
        "on_failure": "abort",
        "timeout_seconds": 300
      }
    }
  },
  "11-install-frontend-deps.sh": {
    "hash": "3291bea337a8bbcabc7f055c37b92c62",
    "data": {
      "role": "utility",
      "description": "Installs npm dependencies for the frontend webapp by running 'npm install' in the frontend directory.",
      "purpose": "Ensures frontend JavaScript dependencies are installed after the frontend code is deployed but before the application starts",
      "execution_order": 75,
      "is_executable": true,
      "is_standalone": false,
      "calls": [
        {
          "file": "04-install-node.sh",
          "method": "exec",
          "when": "conditional",
          "line": 10,
          "critical": false
        }
      ],
      "called_by_likely": [
        "main installation script",
        "deployment pipeline",
        "setup orchestrator"
      ],
      "requires_before": [
        "04-install-node.sh",
        "frontend code deployment",
        "user creation"
      ],
      "produces_after": [
        "frontend build scripts",
        "application startup scripts"
      ],
      "category": "install",
      "inputs": [
        "REMOTE_DIR env var",
        "INSTALL_USER env var",
        "SUDO_USER env var",
        "package.json file",
        "npm command availability"
      ],
      "outputs": [
        "node_modules directory",
        "installed frontend dependencies",
        "package-lock.json"
      ],
      "potential_bugs": [
        {
          "type": "hardcoded_path",
          "description": "Hardcoded path /opt/ids-dashboard may not exist on all systems",
          "severity": "medium",
          "line": 4,
          "suggestion": "Validate REMOTE_DIR exists before using",
          "file": "11-install-frontend-deps.sh"
        },
        {
          "type": "missing_dependency",
          "description": "Script references 04-install-node.sh but doesn't verify it exists",
          "severity": "medium",
          "line": 10,
          "suggestion": "Check if 04-install-node.sh exists before referencing it",
          "file": "11-install-frontend-deps.sh"
        },
        {
          "type": "no_error_handling",
          "description": "npm install can fail but no specific error handling for common npm issues",
          "severity": "medium",
          "line": 13,
          "suggestion": "Add npm cache clean or retry logic for npm install failures",
          "file": "11-install-frontend-deps.sh"
        },
        {
          "type": "missing_env",
          "description": "If USER is not set and SUDO_USER is not set, INSTALL_USER could be empty",
          "severity": "high",
          "line": 5,
          "suggestion": "Add fallback user or validation that INSTALL_USER is not empty",
          "file": "11-install-frontend-deps.sh"
        },
        {
          "type": "race_condition",
          "description": "If multiple instances run simultaneously, npm install could conflict",
          "severity": "low",
          "line": 13,
          "suggestion": "Use npm install with --mutex flag or lock file",
          "file": "11-install-frontend-deps.sh"
        },
        {
          "type": "hardcoded_path",
          "description": "Frontend directory structure assumed to be webapp/frontend",
          "severity": "low",
          "line": 6,
          "suggestion": "Make frontend path configurable",
          "file": "11-install-frontend-deps.sh"
        }
      ],
      "fsm_state": {
        "state_name": "INSTALLING_FRONTEND_DEPS",
        "preconditions": [
          "Node.js and npm installed",
          "frontend code deployed",
          "package.json exists",
          "target user exists"
        ],
        "postconditions": [
          "frontend dependencies installed",
          "node_modules directory created",
          "frontend ready for build"
        ],
        "can_fail": true,
        "on_failure": "abort",
        "timeout_seconds": 300
      }
    }
  },
  "__init__.py": {
    "hash": "2f027674eea10ea2c6e95e1db157c9f1",
    "data": {
      "role": "library",
      "description": "Storage layer package initialization file that exposes database components, models, and schemas for the IDS dashboard. Acts as a public API interface for the storage subsystem.",
      "purpose": "Centralizes and exposes the storage layer's core components (database connections, models, schemas) to other parts of the webapp, providing a clean import interface",
      "execution_order": 15,
      "is_executable": false,
      "is_standalone": false,
      "calls": [
        {
          "file": "webapp/db/storage/database.py",
          "method": "import",
          "when": "startup",
          "line": 4,
          "critical": true
        },
        {
          "file": "webapp/db/storage/models.py",
          "method": "import",
          "when": "startup",
          "line": 5,
          "critical": true
        },
        {
          "file": "webapp/db/storage/schemas.py",
          "method": "import",
          "when": "startup",
          "line": 5,
          "critical": true
        }
      ],
      "called_by_likely": [
        "webapp/main.py",
        "webapp/api/routes.py",
        "webapp/services/*.py",
        "webapp/db/migration_scripts.py",
        "webapp/tests/test_db.py"
      ],
      "requires_before": [
        "webapp/db/storage/database.py",
        "webapp/db/storage/models.py",
        "webapp/db/storage/schemas.py"
      ],
      "produces_after": [
        "webapp/api/endpoints.py",
        "webapp/services/data_service.py",
        "webapp/tests/integration_tests.py"
      ],
      "category": "db",
      "inputs": [
        "webapp/db/storage/database.py module",
        "webapp/db/storage/models.py module",
        "webapp/db/storage/schemas.py module"
      ],
      "outputs": [
        "Exposed Base SQLAlchemy declarative base",
        "Exposed SessionLocal database session factory",
        "Exposed get_session dependency injection function",
        "Exposed init_db database initialization function",
        "Exposed models module for ORM classes",
        "Exposed schemas module for Pydantic models"
      ],
      "potential_bugs": [
        {
          "type": "missing_dependency",
          "description": "No error handling if database.py, models.py, or schemas.py files don't exist or have import errors",
          "severity": "critical",
          "line": 4,
          "suggestion": "Wrap imports in try-catch blocks and provide meaningful error messages for missing dependencies",
          "file": "__init__.py"
        },
        {
          "type": "circular_call",
          "description": "Potential circular import if models.py imports from this __init__.py or if database.py depends on models that import this module",
          "severity": "high",
          "line": 5,
          "suggestion": "Review import dependencies to ensure no circular references between database, models, and this init file",
          "file": "__init__.py"
        },
        {
          "type": "missing_dependency",
          "description": "No validation that imported functions/classes actually exist in their respective modules",
          "severity": "medium",
          "line": 4,
          "suggestion": "Add runtime checks or use explicit import statements to catch AttributeError early",
          "file": "__init__.py"
        },
        {
          "type": "unreachable",
          "description": "If any of the imported modules fail to load, the __all__ list becomes misleading as some items won't be available",
          "severity": "medium",
          "line": 7,
          "suggestion": "Dynamically build __all__ list based on successfully imported components",
          "file": "__init__.py"
        }
      ],
      "fsm_state": {
        "state_name": "CONFIGURING",
        "preconditions": [
          "Python interpreter is running",
          "webapp/db/storage/ directory exists",
          "database.py, models.py, schemas.py files exist in storage directory",
          "Required database libraries (SQLAlchemy, etc.) are installed"
        ],
        "postconditions": [
          "Storage layer components are available for import",
          "Database session management is accessible",
          "ORM models and schemas are exposed",
          "Other modules can import storage functionality"
        ],
        "can_fail": true,
        "on_failure": "abort",
        "timeout_seconds": 5
      }
    }
  },
  "Dockerfile": {
    "hash": "4e76a4ba753276ba2a9c644f04972239",
    "data": {
      "role": "service",
      "description": "Multi-stage Docker container for IDS Agent service that creates a security monitoring system with Suricata for Raspberry Pi deployment.",
      "purpose": "Containerizes the IDS Agent backend service with proper security isolation, dependencies, and runtime configuration",
      "execution_order": 15,
      "is_executable": false,
      "is_standalone": false,
      "calls": [
        {
          "file": "requirements.txt",
          "method": "exec",
          "when": "startup",
          "line": 12,
          "critical": true
        },
        {
          "file": "pyproject.toml",
          "method": "exec",
          "when": "startup",
          "line": 12,
          "critical": true
        },
        {
          "file": "src/",
          "method": "exec",
          "when": "runtime",
          "line": 26,
          "critical": true
        },
        {
          "file": "config.yaml",
          "method": "exec",
          "when": "runtime",
          "line": 27,
          "critical": true
        },
        {
          "file": "ids.app.supervisor",
          "method": "exec",
          "when": "runtime",
          "line": 48,
          "critical": true
        }
      ],
      "called_by_likely": [
        "docker-compose.yml",
        "kubernetes manifests",
        "docker build scripts",
        "deployment scripts"
      ],
      "requires_before": [
        "requirements.txt",
        "pyproject.toml",
        "src/ directory",
        "config.yaml"
      ],
      "produces_after": [
        "containerized IDS service",
        "log directories",
        "data directories"
      ],
      "category": "deploy",
      "inputs": [
        "requirements.txt",
        "pyproject.toml",
        "src/ directory",
        "tests/ directory",
        "config.yaml",
        "base Python 3.11 image"
      ],
      "outputs": [
        "Docker container image",
        "/var/log/ids directory",
        "/var/lib/ids directory",
        "running IDS supervisor process"
      ],
      "potential_bugs": [
        {
          "type": "missing_dependency",
          "description": "config.yaml file may not exist when COPY is executed",
          "severity": "critical",
          "line": 27,
          "suggestion": "Add explicit check or make config.yaml optional with default fallback",
          "file": "Dockerfile"
        },
        {
          "type": "hardcoded_path",
          "description": "Hardcoded /app/config.yaml path in CMD may not match actual config location",
          "severity": "medium",
          "line": 48,
          "suggestion": "Use environment variable for config path or relative path",
          "file": "Dockerfile"
        },
        {
          "type": "no_error_handling",
          "description": "No validation that required source directories exist before COPY",
          "severity": "medium",
          "line": 26,
          "suggestion": "Add build-time checks or use .dockerignore with explicit file listing",
          "file": "Dockerfile"
        },
        {
          "type": "missing_dependency",
          "description": "ids.app.supervisor module may not exist or be importable",
          "severity": "critical",
          "line": 48,
          "suggestion": "Add build-time validation of Python module structure",
          "file": "Dockerfile"
        },
        {
          "type": "race_condition",
          "description": "HEALTHCHECK runs immediately but supervisor may need startup time",
          "severity": "low",
          "line": 45,
          "suggestion": "Implement proper health check endpoint in supervisor module",
          "file": "Dockerfile"
        },
        {
          "type": "missing_env",
          "description": "No environment variables defined for runtime configuration like log levels or network interfaces",
          "severity": "medium",
          "line": 32,
          "suggestion": "Add ENV variables for common configuration overrides",
          "file": "Dockerfile"
        },
        {
          "type": "hardcoded_path",
          "description": "Fixed paths /var/log/ids and /var/lib/ids may conflict with host system",
          "severity": "medium",
          "line": 37,
          "suggestion": "Use configurable volume mount points",
          "file": "Dockerfile"
        }
      ],
      "fsm_state": {
        "state_name": "CONTAINERIZING",
        "preconditions": [
          "source code exists",
          "requirements.txt exists",
          "Docker daemon running",
          "base images available"
        ],
        "postconditions": [
          "IDS Agent container built",
          "non-root user created",
          "directories configured",
          "supervisor ready to start"
        ],
        "can_fail": true,
        "on_failure": "abort",
        "timeout_seconds": 300
      }
    }
  },
  "docker-compose.test.yml": {
    "hash": "9c6f7fd117f003767a2b0b3e70caee5e",
    "data": {
      "role": "test",
      "description": "Docker Compose configuration file that orchestrates multiple containerized test environments for an IDS (Intrusion Detection System) backend including unit tests, integration tests, linting, and coverage reporting.",
      "purpose": "Provides isolated, reproducible test environments for different testing phases of the IDS backend application with proper network isolation and volume management",
      "execution_order": 75,
      "is_executable": false,
      "is_standalone": true,
      "calls": [
        {
          "file": "Dockerfile.test",
          "method": "exec",
          "when": "startup",
          "line": 9,
          "critical": true
        },
        {
          "file": "./src",
          "method": "source",
          "when": "startup",
          "line": 14,
          "critical": true
        },
        {
          "file": "./tests",
          "method": "source",
          "when": "startup",
          "line": 15,
          "critical": true
        },
        {
          "file": "./config.yaml",
          "method": "source",
          "when": "startup",
          "line": 16,
          "critical": true
        }
      ],
      "called_by_likely": [
        "docker-compose",
        "test runner scripts",
        "CI/CD pipelines",
        "make test",
        "npm test"
      ],
      "requires_before": [
        "Dockerfile.test",
        "src/ directory",
        "tests/ directory",
        "config.yaml"
      ],
      "produces_after": [
        "test reports",
        "coverage reports",
        "lint results",
        "junit XML files"
      ],
      "category": "test",
      "inputs": [
        "Dockerfile.test",
        "src/ source code",
        "tests/ test files",
        "config.yaml",
        "Docker engine",
        "Python test dependencies"
      ],
      "outputs": [
        "test-results volume",
        "HTML coverage reports",
        "JUnit XML reports",
        "pytest logs",
        "lint output",
        "test exit codes"
      ],
      "potential_bugs": [
        {
          "type": "missing_dependency",
          "description": "Dockerfile.test might not exist or be properly configured",
          "severity": "critical",
          "line": 9,
          "suggestion": "Ensure Dockerfile.test exists and contains proper test dependencies",
          "file": "docker-compose.test.yml"
        },
        {
          "type": "hardcoded_path",
          "description": "Hardcoded paths like /app/src, /app/tests might not match Dockerfile structure",
          "severity": "high",
          "line": 14,
          "suggestion": "Verify paths match those defined in Dockerfile.test WORKDIR",
          "file": "docker-compose.test.yml"
        },
        {
          "type": "missing_dependency",
          "description": "config.yaml file might not exist causing mount failure",
          "severity": "high",
          "line": 16,
          "suggestion": "Add existence check or make volume optional with :ro,nocopy",
          "file": "docker-compose.test.yml"
        },
        {
          "type": "no_error_handling",
          "description": "flake8 command uses '|| true' which masks all failures",
          "severity": "medium",
          "line": 69,
          "suggestion": "Remove '|| true' or implement proper error handling to fail on critical lint issues",
          "file": "docker-compose.test.yml"
        },
        {
          "type": "race_condition",
          "description": "Multiple services might run simultaneously causing port conflicts or resource contention",
          "severity": "medium",
          "line": 1,
          "suggestion": "Use docker-compose run instead of up for sequential execution",
          "file": "docker-compose.test.yml"
        },
        {
          "type": "missing_env",
          "description": "AWS credentials are hardcoded as 'testing' but AWS services might require different mock values",
          "severity": "low",
          "line": 47,
          "suggestion": "Use proper mock AWS service or document that these are placeholder values",
          "file": "docker-compose.test.yml"
        },
        {
          "type": "hardcoded_path",
          "description": "pytest.log is written to /tmp which might not be accessible in container",
          "severity": "low",
          "line": 22,
          "suggestion": "Write logs to mounted volume like /app/test-results/pytest.log",
          "file": "docker-compose.test.yml"
        },
        {
          "type": "missing_dependency",
          "description": "flake8 is installed at runtime but not in Dockerfile.test, causing repeated downloads",
          "severity": "low",
          "line": 67,
          "suggestion": "Include flake8 in Dockerfile.test to avoid runtime installation",
          "file": "docker-compose.test.yml"
        }
      ],
      "fsm_state": {
        "state_name": "TESTING",
        "preconditions": [
          "Docker engine running",
          "Source code available",
          "Dockerfile.test exists",
          "Test files present"
        ],
        "postconditions": [
          "Test results generated",
          "Coverage reports available",
          "Code quality metrics available",
          "Test containers cleaned up"
        ],
        "can_fail": true,
        "on_failure": "abort",
        "timeout_seconds": 1800
      }
    }
  },
  "deploy.py": {
    "hash": "12187beb0786e6493c4de89b323bbfae",
    "data": {
      "role": "entry_point",
      "description": "Deployment script that uploads code to a Raspberry Pi via SSH/SFTP and configures it as a systemd service. Interactive script that prompts for SSH credentials and deploys the IDS dashboard application.",
      "purpose": "Automates the deployment process of the IDS dashboard web application to a remote Raspberry Pi server, handling file upload, dependency installation, and service configuration.",
      "execution_order": 95,
      "is_executable": true,
      "is_standalone": true,
      "calls": [
        {
          "file": "requirements.txt",
          "method": "exec",
          "when": "runtime",
          "line": 111,
          "critical": true
        },
        {
          "file": "deploy/ids-dashboard.service",
          "method": "source",
          "when": "runtime",
          "line": 116,
          "critical": true
        },
        {
          "file": "secret.json",
          "method": "source",
          "when": "startup",
          "line": 25,
          "critical": false
        }
      ],
      "called_by_likely": [
        "manual execution",
        "CI/CD pipeline",
        "deployment scripts"
      ],
      "requires_before": [
        "webapp/backend/requirements.txt",
        "webapp/backend/deploy/ids-dashboard.service",
        "application code"
      ],
      "produces_after": [
        "running systemd service",
        "deployed application"
      ],
      "category": "deploy",
      "inputs": [
        "SSH host IP",
        "SSH username",
        "SSH password",
        "sudo password",
        "local repository files",
        "requirements.txt",
        "systemd service file"
      ],
      "outputs": [
        "secret.json file",
        "deployed code on remote server",
        "configured systemd service",
        "running IDS dashboard service"
      ],
      "potential_bugs": [
        {
          "type": "missing_dependency",
          "description": "paramiko import not in requirements check",
          "severity": "high",
          "line": 8,
          "suggestion": "Add paramiko to requirements.txt or check if installed",
          "file": "deploy.py"
        },
        {
          "type": "hardcoded_path",
          "description": "REMOTE_DIR hardcoded to /opt/ids-dashboard without checking permissions",
          "severity": "medium",
          "line": 17,
          "suggestion": "Make REMOTE_DIR configurable or check write permissions",
          "file": "deploy.py"
        },
        {
          "type": "no_error_handling",
          "description": "SSH connection timeout but no retry mechanism",
          "severity": "medium",
          "line": 30,
          "suggestion": "Add retry logic for SSH connection failures",
          "file": "deploy.py"
        },
        {
          "type": "missing_dependency",
          "description": "requirements.txt path not verified before pip install",
          "severity": "high",
          "line": 111,
          "suggestion": "Check if requirements.txt exists before pip install command",
          "file": "deploy.py"
        },
        {
          "type": "missing_dependency",
          "description": "systemd service file path not verified before copy",
          "severity": "critical",
          "line": 116,
          "suggestion": "Verify service file exists at deploy/ids-dashboard.service before copying",
          "file": "deploy.py"
        },
        {
          "type": "hardcoded_path",
          "description": "Python3 command assumed to exist on remote system",
          "severity": "medium",
          "line": 111,
          "suggestion": "Check if python3 is available or use python fallback",
          "file": "deploy.py"
        },
        {
          "type": "no_error_handling",
          "description": "No rollback mechanism if deployment fails partially",
          "severity": "medium",
          "line": 95,
          "suggestion": "Add rollback functionality to restore previous state on failure",
          "file": "deploy.py"
        },
        {
          "type": "missing_env",
          "description": "SSH host key verification uses AutoAddPolicy without user consent",
          "severity": "medium",
          "line": 31,
          "suggestion": "Warn user about host key auto-acceptance or make configurable",
          "file": "deploy.py"
        },
        {
          "type": "no_error_handling",
          "description": "Secret file written with passwords in plain text without permission check",
          "severity": "high",
          "line": 25,
          "suggestion": "Set restrictive file permissions (600) on secret.json",
          "file": "deploy.py"
        },
        {
          "type": "race_condition",
          "description": "Service restart without checking if previous instance stopped",
          "severity": "low",
          "line": 121,
          "suggestion": "Add service status check before restart",
          "file": "deploy.py"
        }
      ],
      "fsm_state": {
        "state_name": "DEPLOYING",
        "preconditions": [
          "local repository exists",
          "target server accessible via SSH",
          "required files present"
        ],
        "postconditions": [
          "code deployed",
          "service running",
          "dashboard accessible on port 8080"
        ],
        "can_fail": true,
        "on_failure": "abort",
        "timeout_seconds": 300
      }
    }
  },
  "config.yaml": {
    "hash": "511d79fc88b42261c1d37d5878b47534",
    "data": {
      "role": "config",
      "description": "Main configuration file for a Suricata IDS monitoring system. Defines settings for AWS OpenSearch, Raspberry Pi sensor, monitoring tools (Vector, Grafana, Prometheus), and network configuration.",
      "purpose": "Centralized configuration management for the entire IDS infrastructure, providing structured settings for all system components including cloud services, network monitoring, and Docker services",
      "execution_order": 5,
      "is_executable": false,
      "is_standalone": false,
      "calls": [],
      "called_by_likely": [
        "main.py",
        "app.py",
        "config_loader.py",
        "docker-compose.yml",
        "vector/vector.toml"
      ],
      "requires_before": [],
      "produces_after": [
        "docker/docker-compose.yml",
        "vector/vector.toml",
        "suricata/suricata.yaml"
      ],
      "category": "config",
      "inputs": [
        "secret.json (for AWS credentials and Tailscale keys)",
        "environment variables",
        "file system paths"
      ],
      "outputs": [
        "configuration values for all system components",
        "service discovery parameters",
        "resource limits"
      ],
      "potential_bugs": [
        {
          "type": "hardcoded_path",
          "description": "Hardcoded path '/mnt/ram_logs/eve.json' may not exist if RAM disk is not properly mounted",
          "severity": "critical",
          "line": 24,
          "suggestion": "Add validation for RAM disk mount point or make path configurable with fallback",
          "file": "config.yaml"
        },
        {
          "type": "hardcoded_path",
          "description": "Static IP address '192.168.178.66' and network '192.168.178.0/24' are hardcoded for specific environment",
          "severity": "high",
          "line": 8,
          "suggestion": "Make network configuration dynamic or environment-specific",
          "file": "config.yaml"
        },
        {
          "type": "missing_dependency",
          "description": "References secret.json file for AWS and Tailscale credentials but file existence is not validated",
          "severity": "critical",
          "line": 6,
          "suggestion": "Add validation to ensure secret.json exists and contains required keys",
          "file": "config.yaml"
        },
        {
          "type": "hardcoded_path",
          "description": "OpenSearch endpoint is hardcoded to specific AWS domain, not environment-agnostic",
          "severity": "medium",
          "line": 4,
          "suggestion": "Use environment variables or separate config files for different deployment environments",
          "file": "config.yaml"
        },
        {
          "type": "missing_env",
          "description": "Tailscale configuration is commented out but no fallback or validation for required fields",
          "severity": "medium",
          "line": 77,
          "suggestion": "Provide clear documentation on which Tailscale fields are required and validate presence",
          "file": "config.yaml"
        },
        {
          "type": "race_condition",
          "description": "Docker services start order not specified, but some services depend on others (Vector depends on Redis/OpenSearch)",
          "severity": "medium",
          "line": 57,
          "suggestion": "Add depends_on configuration or startup delay mechanisms",
          "file": "config.yaml"
        },
        {
          "type": "no_error_handling",
          "description": "No validation rules for resource limits (CPU/RAM percentages could exceed 100%)",
          "severity": "low",
          "line": 12,
          "suggestion": "Add configuration validation to ensure percentage values are between 0-100",
          "file": "config.yaml"
        }
      ],
      "fsm_state": {
        "state_name": "CONFIGURING",
        "preconditions": [
          "File system is accessible",
          "YAML parser is available"
        ],
        "postconditions": [
          "All service configurations are loaded",
          "System parameters are defined",
          "Service dependencies are mapped"
        ],
        "can_fail": true,
        "on_failure": "abort",
        "timeout_seconds": 5
      }
    }
  },
  "main.py": {
    "hash": "92001944594e8b2d31d7a9d1b2ade04f",
    "data": {
      "role": "entry_point",
      "description": "Main deployment script for a webapp backend that handles SSH deployment to Raspberry Pi and AWS OpenSearch configuration. Manages configuration loading, environment setup, and remote deployment operations.",
      "purpose": "Serves as the primary entry point for deploying the webapp backend to remote systems with configuration management and deployment utilities",
      "execution_order": 15,
      "is_executable": true,
      "is_standalone": true,
      "calls": [
        {
          "file": "src/ids/deploy/pi_uploader.py",
          "method": "import",
          "when": "startup",
          "line": 17,
          "critical": false
        },
        {
          "file": "src/ids/deploy/opensearch_domain.py",
          "method": "import",
          "when": "startup",
          "line": 21,
          "critical": false
        },
        {
          "file": "src/ids/config/loader.py",
          "method": "import",
          "when": "startup",
          "line": 26,
          "critical": false
        },
        {
          "file": "subprocess",
          "method": "subprocess",
          "when": "runtime",
          "line": 81,
          "critical": true
        }
      ],
      "called_by_likely": [
        "deployment scripts",
        "CI/CD pipelines",
        "manual execution",
        "docker containers"
      ],
      "requires_before": [
        "config files",
        "secret files",
        "SSH keys",
        "network connectivity"
      ],
      "produces_after": [
        "environment files",
        "deployed services",
        "remote configurations"
      ],
      "category": "deploy",
      "inputs": [
        "config.yaml",
        "secrets.json",
        "SSH keys",
        "command line arguments",
        "AWS credentials"
      ],
      "outputs": [
        ".env files",
        "deployed backend services",
        "remote configurations",
        "SSH connections"
      ],
      "potential_bugs": [
        {
          "type": "missing_dependency",
          "description": "Optional imports with broad Exception catching could hide ImportError issues",
          "severity": "medium",
          "line": 17,
          "suggestion": "Catch specific ImportError instead of broad Exception",
          "file": "main.py"
        },
        {
          "type": "missing_error_handling",
          "description": "SSH operations don't validate host connectivity before attempting commands",
          "severity": "high",
          "line": 95,
          "suggestion": "Add connection testing before SSH operations",
          "file": "main.py"
        },
        {
          "type": "hardcoded_path",
          "description": "Default remote_dir hardcoded to /opt/ids2 without validation",
          "severity": "medium",
          "line": 35,
          "suggestion": "Make remote directory configurable and validate existence",
          "file": "main.py"
        },
        {
          "type": "no_error_handling",
          "description": "File operations in load_yaml_data and load_json_data catch all exceptions silently",
          "severity": "medium",
          "line": 168,
          "suggestion": "Add specific exception handling and logging for file operations",
          "file": "main.py"
        },
        {
          "type": "missing_env",
          "description": "sys.path modification assumes SRC_ROOT exists without validation",
          "severity": "high",
          "line": 14,
          "suggestion": "Validate SRC_ROOT exists before adding to sys.path",
          "file": "main.py"
        },
        {
          "type": "race_condition",
          "description": "Concurrent SSH operations could conflict if multiple instances run simultaneously",
          "severity": "medium",
          "line": 95,
          "suggestion": "Add file locking or deployment state management",
          "file": "main.py"
        },
        {
          "type": "unreachable",
          "description": "Code after line 207 is cut off, potential incomplete implementation",
          "severity": "critical",
          "line": 207,
          "suggestion": "Complete the render_env_file function implementation",
          "file": "main.py"
        }
      ],
      "fsm_state": {
        "state_name": "DEPLOYING",
        "preconditions": [
          "config files exist",
          "SSH connectivity available",
          "credentials configured"
        ],
        "postconditions": [
          "backend deployed",
          "services running",
          "environment configured"
        ],
        "can_fail": true,
        "on_failure": "abort",
        "timeout_seconds": 300
      }
    }
  },
  "database.py": {
    "hash": "f204dff87dfd21c6f5008cd2975863cf",
    "data": {
      "role": "library",
      "description": "Database configuration and session management library for an IDS dashboard application using SQLAlchemy ORM with SQLite as default database.",
      "purpose": "Provides centralized database engine, session factory, and connection management for the IDS dashboard web application",
      "execution_order": 15,
      "is_executable": false,
      "is_standalone": false,
      "calls": [
        {
          "file": "sqlalchemy",
          "method": "import",
          "when": "startup",
          "line": 8,
          "critical": true
        },
        {
          "file": "pathlib",
          "method": "import",
          "when": "startup",
          "line": 6,
          "critical": true
        },
        {
          "file": "os",
          "method": "import",
          "when": "runtime",
          "line": 22,
          "critical": false
        }
      ],
      "called_by_likely": [
        "webapp/api/routes.py",
        "webapp/models/*.py",
        "webapp/main.py",
        "webapp/services/*.py"
      ],
      "requires_before": [
        "data/ directory creation",
        "DATABASE_URL environment variable (optional)"
      ],
      "produces_after": [
        "webapp/models/*.py",
        "webapp/api/endpoints/*.py",
        "database migration scripts"
      ],
      "category": "db",
      "inputs": [
        "DATABASE_URL environment variable",
        "data/ directory",
        "SQLAlchemy models"
      ],
      "outputs": [
        "database engine instance",
        "session factory",
        "database tables",
        "data/ids_dashboard.db file"
      ],
      "potential_bugs": [
        {
          "type": "race_condition",
          "description": "Multiple processes could try to create data/ directory simultaneously",
          "severity": "low",
          "line": 33,
          "suggestion": "Add file locking or check-and-create atomically",
          "file": "database.py"
        },
        {
          "type": "no_error_handling",
          "description": "No exception handling in get_session() for database connection failures",
          "severity": "medium",
          "line": 37,
          "suggestion": "Add try/except around SessionLocal() creation",
          "file": "database.py"
        },
        {
          "type": "no_error_handling",
          "description": "_build_engine() doesn't handle database connection errors",
          "severity": "high",
          "line": 21,
          "suggestion": "Add try/except around create_engine() calls",
          "file": "database.py"
        },
        {
          "type": "hardcoded_path",
          "description": "Hardcoded 'data' directory path may not exist in all deployment environments",
          "severity": "medium",
          "line": 33,
          "suggestion": "Make data directory configurable via environment variable",
          "file": "database.py"
        },
        {
          "type": "missing_dependency",
          "description": "SQLAlchemy import not in requirements check",
          "severity": "medium",
          "line": 8,
          "suggestion": "Ensure SQLAlchemy is in requirements.txt",
          "file": "database.py"
        },
        {
          "type": "unreachable",
          "description": "_resolve_db_url() function is defined but its logic seems flawed - it replaces a relative path with absolute but uses the same relative path",
          "severity": "medium",
          "line": 16,
          "suggestion": "Fix path resolution logic or remove if unused",
          "file": "database.py"
        }
      ],
      "fsm_state": {
        "state_name": "CONFIGURING",
        "preconditions": [
          "SQLAlchemy installed",
          "filesystem write permissions",
          "Python environment ready"
        ],
        "postconditions": [
          "database engine configured",
          "session factory available",
          "database file created",
          "tables initialized"
        ],
        "can_fail": true,
        "on_failure": "abort",
        "timeout_seconds": 30
      }
    }
  },
  "models.py": {
    "hash": "26f48646a9431379f584c5d37194c7c2",
    "data": {
      "role": "library",
      "description": "Defines SQLAlchemy ORM models for IDS dashboard configuration and telemetry data storage. Contains database table schemas for various service configurations including AWS, Raspberry Pi, Suricata, Vector, Redis, Prometheus, and Grafana.",
      "purpose": "Provides the database schema and model definitions that other parts of the application use to store and retrieve configuration data for the IDS (Intrusion Detection System) dashboard",
      "execution_order": 15,
      "is_executable": false,
      "is_standalone": false,
      "calls": [
        {
          "file": "webapp/db/storage/database.py",
          "method": "import",
          "when": "startup",
          "line": 9,
          "critical": true
        }
      ],
      "called_by_likely": [
        "webapp/db/storage/database.py",
        "webapp/api/*.py",
        "webapp/services/*.py",
        "migration scripts",
        "configuration management scripts"
      ],
      "requires_before": [
        "webapp/db/storage/database.py"
      ],
      "produces_after": [
        "database migration scripts",
        "API endpoints",
        "configuration services"
      ],
      "category": "db",
      "inputs": [
        "SQLAlchemy Base class from database.py",
        "Python datetime module"
      ],
      "outputs": [
        "Database table schemas",
        "ORM model classes",
        "Timestamp mixin for auditing"
      ],
      "potential_bugs": [
        {
          "type": "missing_dependency",
          "description": "File appears to be truncated - DockerConfig class definition is incomplete",
          "severity": "critical",
          "line": 115,
          "suggestion": "Complete the DockerConfig class definition and ensure file is not corrupted",
          "file": "models.py"
        },
        {
          "type": "hardcoded_path",
          "description": "Multiple hardcoded default paths that may not exist on target systems (/mnt/ram_logs/eve.json, suricata/suricata.yaml, suricata/rules)",
          "severity": "high",
          "line": 45,
          "suggestion": "Make paths configurable through environment variables or validate existence",
          "file": "models.py"
        },
        {
          "type": "no_error_handling",
          "description": "No validation constraints on critical fields like IP addresses, ports, or file paths",
          "severity": "medium",
          "line": 35,
          "suggestion": "Add CheckConstraints for IP address formats, port ranges, and path validation",
          "file": "models.py"
        },
        {
          "type": "missing_dependency",
          "description": "Uses datetime.utcnow which is deprecated in Python 3.12+",
          "severity": "medium",
          "line": 14,
          "suggestion": "Use datetime.now(timezone.utc) instead of datetime.utcnow",
          "file": "models.py"
        },
        {
          "type": "missing_env",
          "description": "Secrets stored in database without encryption - sensitive data like AWS keys, passwords stored as plain text",
          "severity": "critical",
          "line": 18,
          "suggestion": "Implement field-level encryption for sensitive data or use external secret management",
          "file": "models.py"
        },
        {
          "type": "no_error_handling",
          "description": "No unique constraints on configuration tables - could lead to multiple conflicting configurations",
          "severity": "medium",
          "line": 30,
          "suggestion": "Add unique constraints or implement singleton pattern for configuration tables",
          "file": "models.py"
        }
      ],
      "fsm_state": {
        "state_name": "CONFIGURING",
        "preconditions": [
          "Database connection established",
          "SQLAlchemy Base class available",
          "Database schema not yet created"
        ],
        "postconditions": [
          "Database models defined",
          "Table schemas available for migration",
          "ORM classes ready for use"
        ],
        "can_fail": true,
        "on_failure": "abort",
        "timeout_seconds": 10
      }
    }
  },
  "crud.py": {
    "hash": "e4c15684e4472c675163f938c88abbf9",
    "data": {
      "role": "library",
      "description": "Provides CRUD utility functions for database operations, specifically for dashboard configuration management. Contains helper functions for singleton pattern implementation and model updates.",
      "purpose": "Abstracts common database operations like get-or-create singleton instances and updating model attributes from dictionaries, reducing code duplication across the web application.",
      "execution_order": 40,
      "is_executable": false,
      "is_standalone": false,
      "calls": [
        {
          "file": "webapp/db/storage/database.py",
          "method": "import",
          "when": "startup",
          "line": 8,
          "critical": true
        },
        {
          "file": "sqlalchemy/orm/__init__.py",
          "method": "import",
          "when": "startup",
          "line": 6,
          "critical": true
        }
      ],
      "called_by_likely": [
        "webapp/api/dashboard.py",
        "webapp/services/config.py",
        "webapp/controllers/settings.py",
        "webapp/models/dashboard.py"
      ],
      "requires_before": [
        "webapp/db/storage/database.py",
        "database connection setup",
        "SQLAlchemy models definition"
      ],
      "produces_after": [
        "dashboard configuration endpoints",
        "settings management views",
        "configuration API services"
      ],
      "category": "db",
      "inputs": [
        "SQLAlchemy Session objects",
        "SQLAlchemy model classes",
        "dictionary payloads for updates",
        "database connection"
      ],
      "outputs": [
        "model instances",
        "database state changes",
        "committed transactions"
      ],
      "potential_bugs": [
        {
          "type": "no_error_handling",
          "description": "get_or_create_singleton has no error handling for database exceptions during commit/refresh operations",
          "severity": "high",
          "line": 15,
          "suggestion": "Add try-catch block around session.commit() and session.refresh() calls",
          "file": "crud.py"
        },
        {
          "type": "no_error_handling",
          "description": "update_model doesn't validate payload keys or handle attribute setting errors",
          "severity": "medium",
          "line": 23,
          "suggestion": "Add validation for allowed attributes and try-catch for setattr operations",
          "file": "crud.py"
        },
        {
          "type": "race_condition",
          "description": "get_or_create_singleton could create duplicate instances in concurrent environments between query and create",
          "severity": "high",
          "line": 13,
          "suggestion": "Use database-level unique constraints and handle IntegrityError exceptions",
          "file": "crud.py"
        },
        {
          "type": "missing_dependency",
          "description": "No explicit validation that the model parameter extends Base class at runtime",
          "severity": "medium",
          "line": 12,
          "suggestion": "Add isinstance check to ensure model is subclass of Base",
          "file": "crud.py"
        },
        {
          "type": "no_error_handling",
          "description": "Session is not closed or rolled back on errors, potential resource leak",
          "severity": "medium",
          "line": 15,
          "suggestion": "Use context managers or explicit session cleanup in error cases",
          "file": "crud.py"
        }
      ],
      "fsm_state": {
        "state_name": "DATABASE_OPERATIONS",
        "preconditions": [
          "database connection established",
          "SQLAlchemy models defined",
          "active database session"
        ],
        "postconditions": [
          "model instances created or updated",
          "database state synchronized",
          "session committed"
        ],
        "can_fail": true,
        "on_failure": "abort",
        "timeout_seconds": 30
      }
    }
  },
  "schemas.py": {
    "hash": "d11627615f5aa460120c7cc300a08112",
    "data": {
      "role": "library",
      "description": "Defines Pydantic schemas for dashboard configuration, providing type validation and serialization for various system components. Contains schema classes for secrets, AWS, Raspberry Pi, Suricata, Vector, Tailscale, and FastAPI configurations.",
      "purpose": "Provides type-safe configuration models with validation for the dashboard application, ensuring data integrity and proper structure for system configurations",
      "execution_order": 10,
      "is_executable": false,
      "is_standalone": false,
      "calls": [
        {
          "file": "pydantic",
          "method": "import",
          "when": "startup",
          "line": 6,
          "critical": true
        },
        {
          "file": "__future__",
          "method": "import",
          "when": "startup",
          "line": 4,
          "critical": true
        }
      ],
      "called_by_likely": [
        "webapp/api/endpoints.py",
        "webapp/config/settings.py",
        "webapp/main.py",
        "webapp/services/config_service.py"
      ],
      "requires_before": [
        "pydantic package installation",
        "python environment setup"
      ],
      "produces_after": [
        "webapp/services/config_service.py",
        "webapp/api/config_endpoints.py",
        "webapp/models/config.py"
      ],
      "category": "db",
      "inputs": [
        "pydantic library",
        "typing annotations",
        "python runtime"
      ],
      "outputs": [
        "configuration schema classes",
        "type validation",
        "serialization models"
      ],
      "potential_bugs": [
        {
          "type": "hardcoded_path",
          "description": "Hardcoded paths like '/mnt/ram_logs/eve.json' and 'suricata/suricata.yaml' may not exist on all systems",
          "severity": "medium",
          "line": 40,
          "suggestion": "Make paths configurable or validate existence at runtime",
          "file": "schemas.py"
        },
        {
          "type": "hardcoded_path",
          "description": "Default network '192.168.178.0/24' is hardcoded and may not match actual network topology",
          "severity": "low",
          "line": 25,
          "suggestion": "Auto-detect network or make configurable",
          "file": "schemas.py"
        },
        {
          "type": "missing_dependency",
          "description": "No validation that AWS region 'eu-central-1' is valid or accessible",
          "severity": "low",
          "line": 20,
          "suggestion": "Add AWS region validation",
          "file": "schemas.py"
        },
        {
          "type": "no_error_handling",
          "description": "No validation for IP address format in pi_ip field",
          "severity": "medium",
          "line": 24,
          "suggestion": "Add IP address format validation using pydantic validators",
          "file": "schemas.py"
        },
        {
          "type": "no_error_handling",
          "description": "Port numbers not validated for valid range (1-65535)",
          "severity": "medium",
          "line": 80,
          "suggestion": "Add port range validation using Field constraints",
          "file": "schemas.py"
        },
        {
          "type": "missing_env",
          "description": "Secret fields are optional but no indication of which are required for functionality",
          "severity": "medium",
          "line": 10,
          "suggestion": "Add documentation or validation for required secret combinations",
          "file": "schemas.py"
        },
        {
          "type": "hardcoded_path",
          "description": "Log paths assume specific mount points that may not exist",
          "severity": "high",
          "line": 40,
          "suggestion": "Add path existence validation or make paths environment-dependent",
          "file": "schemas.py"
        }
      ],
      "fsm_state": {
        "state_name": "SCHEMA_DEFINED",
        "preconditions": [
          "pydantic installed",
          "python imports available"
        ],
        "postconditions": [
          "configuration schemas available for validation",
          "type hints ready for runtime"
        ],
        "can_fail": true,
        "on_failure": "abort",
        "timeout_seconds": 5
      }
    }
  },
  "conftest.py": {
    "hash": "88c7879e7938068ab5a2962f22b333fb",
    "data": {
      "role": "config",
      "description": "Global pytest configuration file that centralizes reusable fixtures, hooks, and test configuration for the entire test suite. Sets up logging, temporary directories, event loops, and imports specialized fixtures from other modules.",
      "purpose": "To provide a centralized testing infrastructure that automatically configures pytest behavior, manages test resources, and makes shared fixtures available to all test modules in the project",
      "execution_order": 5,
      "is_executable": false,
      "is_standalone": false,
      "calls": [
        {
          "file": "fixtures/alerte_fixtures.py",
          "method": "import",
          "when": "startup",
          "line": 15,
          "critical": true
        },
        {
          "file": "fixtures/config_fixtures.py",
          "method": "import",
          "when": "startup",
          "line": 19,
          "critical": true
        },
        {
          "file": "fixtures/container_fixtures.py",
          "method": "import",
          "when": "startup",
          "line": 22,
          "critical": true
        },
        {
          "file": "fixtures/infra_fixtures.py",
          "method": "import",
          "when": "startup",
          "line": 135,
          "critical": true
        }
      ],
      "called_by_likely": [
        "pytest",
        "all test files in the project",
        "test runners",
        "CI/CD pipelines"
      ],
      "requires_before": [
        "fixtures/alerte_fixtures.py",
        "fixtures/config_fixtures.py",
        "fixtures/container_fixtures.py",
        "fixtures/infra_fixtures.py"
      ],
      "produces_after": [
        "all test files that depend on these fixtures"
      ],
      "category": "test",
      "inputs": [
        "pytest configuration",
        "fixture modules",
        "test data directory",
        "temporary path factory"
      ],
      "outputs": [
        "global test fixtures",
        "pytest configuration",
        "logging setup",
        "temporary directories",
        "async event loops"
      ],
      "potential_bugs": [
        {
          "type": "missing_dependency",
          "description": "Import statements for fixture modules could fail if those files don't exist",
          "severity": "critical",
          "line": 15,
          "suggestion": "Add try-catch blocks around imports or ensure fixture files exist",
          "file": "conftest.py"
        },
        {
          "type": "no_error_handling",
          "description": "Event loop fixture doesn't handle loop creation failure properly",
          "severity": "medium",
          "line": 49,
          "suggestion": "Add proper exception handling for asyncio loop operations",
          "file": "conftest.py"
        },
        {
          "type": "dead_code",
          "description": "Empty pass statements in pytest hooks serve no purpose",
          "severity": "low",
          "line": 81,
          "suggestion": "Either implement the hooks or remove them",
          "file": "conftest.py"
        },
        {
          "type": "race_condition",
          "description": "Cleanup fixtures might conflict if tests run in parallel and share temp directories",
          "severity": "medium",
          "line": 73,
          "suggestion": "Ensure temp directories are uniquely named per test",
          "file": "conftest.py"
        },
        {
          "type": "missing_dependency",
          "description": "shutil import is done inside fixture function, could fail at runtime",
          "severity": "medium",
          "line": 74,
          "suggestion": "Move shutil import to module level",
          "file": "conftest.py"
        },
        {
          "type": "unreachable",
          "description": "Logic in pytest_collection_modifyitems for 'slow' marker check seems incorrect",
          "severity": "medium",
          "line": 93,
          "suggestion": "Fix marker detection logic: item.get_closest_marker('slow') returns marker object, not string",
          "file": "conftest.py"
        }
      ],
      "fsm_state": {
        "state_name": "CONFIGURING",
        "preconditions": [
          "pytest framework installed",
          "fixture modules available",
          "test environment initialized"
        ],
        "postconditions": [
          "pytest configured with custom fixtures",
          "logging configured",
          "async support enabled",
          "temp directories available"
        ],
        "can_fail": true,
        "on_failure": "abort",
        "timeout_seconds": 30
      }
    }
  },
  "ids-dashboard.service": {
    "hash": "2f7cb9239d0f8e925969b31034e69d0f",
    "data": {
      "role": "service",
      "description": "SystemD service unit file that defines how to run the IDS Dashboard as a system service. Configures service startup, restart behavior, and environment settings.",
      "purpose": "Enables the IDS Dashboard application to run as a managed system service with automatic restart capabilities and proper service lifecycle management",
      "execution_order": 95,
      "is_executable": false,
      "is_standalone": false,
      "calls": [
        {
          "file": "/usr/bin/python3",
          "method": "exec",
          "when": "startup",
          "line": 7,
          "critical": true
        },
        {
          "file": "ids.dashboard.main",
          "method": "import",
          "when": "startup",
          "line": 7,
          "critical": true
        }
      ],
      "called_by_likely": [
        "systemctl",
        "service",
        "deployment scripts",
        "install scripts"
      ],
      "requires_before": [
        "ids.dashboard.main module",
        "python3 installation",
        "network.target"
      ],
      "produces_after": [
        "running IDS dashboard service",
        "system service registration"
      ],
      "category": "deploy",
      "inputs": [
        "/opt/ids-dashboard directory",
        "ids.dashboard.main Python module",
        "/usr/bin/python3",
        "network.target service",
        "PYTHONUNBUFFERED environment variable"
      ],
      "outputs": [
        "running system service",
        "service logs",
        "automatic service restart on failure",
        "multi-user.target dependency"
      ],
      "potential_bugs": [
        {
          "type": "hardcoded_path",
          "description": "WorkingDirectory hardcoded to /opt/ids-dashboard may not exist during deployment",
          "severity": "high",
          "line": 6,
          "suggestion": "Ensure deployment scripts create this directory or make path configurable",
          "file": "ids-dashboard.service"
        },
        {
          "type": "missing_dependency",
          "description": "No verification that ids.dashboard.main module exists before service start",
          "severity": "high",
          "line": 7,
          "suggestion": "Add ExecStartPre to verify module availability",
          "file": "ids-dashboard.service"
        },
        {
          "type": "hardcoded_path",
          "description": "Python path /usr/bin/python3 may not exist on all systems",
          "severity": "medium",
          "line": 7,
          "suggestion": "Use /usr/bin/env python3 or verify python3 location during installation",
          "file": "ids-dashboard.service"
        },
        {
          "type": "missing_dependency",
          "description": "No User/Group specified - service runs as root by default",
          "severity": "medium",
          "line": 5,
          "suggestion": "Add User= and Group= directives for security",
          "file": "ids-dashboard.service"
        },
        {
          "type": "no_error_handling",
          "description": "No ExecStartPre checks for prerequisites like directory existence or module availability",
          "severity": "medium",
          "line": 5,
          "suggestion": "Add ExecStartPre=/bin/bash -c 'test -d /opt/ids-dashboard'",
          "file": "ids-dashboard.service"
        },
        {
          "type": "missing_dependency",
          "description": "RestartSec of 5 seconds may be too aggressive for certain failure modes",
          "severity": "low",
          "line": 9,
          "suggestion": "Consider exponential backoff or longer initial delay",
          "file": "ids-dashboard.service"
        },
        {
          "type": "missing_env",
          "description": "Only PYTHONUNBUFFERED is set - application may need additional environment variables",
          "severity": "low",
          "line": 10,
          "suggestion": "Review application requirements for additional environment variables",
          "file": "ids-dashboard.service"
        }
      ],
      "fsm_state": {
        "state_name": "DEPLOYING",
        "preconditions": [
          "systemd is available",
          "/opt/ids-dashboard directory exists",
          "ids.dashboard.main module is installed",
          "python3 is available",
          "network.target is available"
        ],
        "postconditions": [
          "IDS Dashboard service is registered",
          "service can be started with systemctl",
          "service will auto-restart on failure",
          "service runs after network is available"
        ],
        "can_fail": true,
        "on_failure": "abort",
        "timeout_seconds": 30
      }
    }
  }
}